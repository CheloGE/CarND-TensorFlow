{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The aim of this notebook is to implement a CNN in tensor flow\n",
    "\n",
    "First let's create a simple CNN layer as shown below:\n",
    "![](figures/figure7.jpg)\n",
    "Even though we won't run it, this will help us understanding the concept of a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the structure\n",
    "image_width = 32\n",
    "image_height = 32\n",
    "input_channels = 3\n",
    "kernel_width = 3\n",
    "kernel_height = 3\n",
    "k_kernels = 20\n",
    "vertical_stride = 2\n",
    "horizontal_stride = 2\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, image_height, image_width, input_channels])\n",
    "weight = tf.Variable(tf.truncated_normal([kernel_height, kernel_width, input_channels, k_kernels]))\n",
    "bias = tf.Variable(tf.zeros(k_kernels))\n",
    "conv_layer = tf.nn.conv2d(x, filter=weight, strides=[1, vertical_stride, horizontal_stride, 1], padding='SAME')\n",
    "conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "conv_layer = tf.nn.relu(conv_layer) # RELU activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/figure8.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying a maxpool layer to the above structure\n",
    "conv_layer = tf.nn.max_pool(conv_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/figure9.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a LeNet CNN network to solve Mnist\n",
    "\n",
    "We will use a LeNet architecture to predecit numbers in the Mnist layer. The LeNet network has the following architecture:\n",
    "\n",
    "![](figures/figure10.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /datasets/ud730/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# reading mnist data\n",
    "mnist_data = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True, reshape=False)  \n",
    "train_data = mnist_data.train.images.astype(np.float32)\n",
    "test_data = mnist_data.test.images.astype(np.float32)\n",
    "train_labels = mnist_data.train.labels.astype(np.float32)\n",
    "test_labels = mnist_data.test.labels.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset size: (55000, 28, 28, 1)\n",
      "testing dataset size: (10000, 28, 28, 1)\n",
      "labels trainig dataset size: (55000, 10)\n",
      "labels testing dataset size: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "#Visualize shape of data\n",
    "print(f\"training dataset size: {train_data.shape}\")\n",
    "print(f\"testing dataset size: {test_data.shape}\")\n",
    "print(f\"labels trainig dataset size: {train_labels.shape}\")\n",
    "print(f\"labels testing dataset size: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(input_layer, weights, bias, stride=[1,1,1,1], pad='SAME', name=None):\n",
    "    \"\"\"\n",
    "        This function aims to return a convolution layer based on the inputs given\n",
    "        \n",
    "        @input input_layer: input features maps or pixels\n",
    "        @input weights: weights to perform the convolution\n",
    "        @input bias: bias to perform the convolution\n",
    "        @input stride: stride to perform in the order [batch, vertical_stride, horizontal_stride, \n",
    "                                                                                channels or dept strides]\n",
    "        @input pad: could be SAME or VALID\n",
    "        \n",
    "        @output convLayer: convolution layer with a RELU activation funtion at the end\n",
    "    \"\"\"\n",
    "    if name==None:\n",
    "        convLayer = tf.nn.conv2d(input_layer, filter=weights, strides=stride, padding=pad)\n",
    "    else:\n",
    "        convLayer = tf.nn.conv2d(input_layer, filter=weights, strides=stride, padding=pad, name=name)\n",
    "    convLayer = tf.nn.bias_add(convLayer, bias)\n",
    "    convLayer = tf.nn.relu(convLayer) # RELU activation function\n",
    "    return convLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling_layer(input_layer, filter_size = [1, 2, 2, 1], stride = [1, 2, 2, 1], pad = 'SAME', name=None):\n",
    "    \"\"\"\n",
    "        This function performs a pooling to the input layer based on the parameters below\n",
    "        \n",
    "        @input input_layer: input feature maps or pixels\n",
    "        @input filter_size: dimensions of the kernel to perform pooling [batch, filter_height, filter_width, \n",
    "                                                                                                      filter_depth]\n",
    "        @input stride: stride to perform in the order [batch, vertical_stride, horizontal_stride, \n",
    "                                                                                channels or dept strides]                                                                                \n",
    "        @input pad: could be SAME or VALID\n",
    "        \n",
    "        @output pool_layer: pooling layer result structure.\n",
    "    \"\"\"\n",
    "    if name==None:\n",
    "        pool_layer = tf.nn.max_pool(input_layer, ksize=filter_size, strides=stride, padding= pad)\n",
    "    else:\n",
    "        pool_layer = tf.nn.max_pool(input_layer, ksize=filter_size, strides=stride, padding= pad, name=name)\n",
    "    return pool_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 10\n",
    "batch_size = 128  # Decrease batch size if you don't have enough memory\n",
    "display_step = 1\n",
    "\n",
    "n_input = train_data.shape[1:4]  # MNIST data input (img shape: 28x28x1)\n",
    "n_classes = train_labels.shape[1]  # MNIST total classes (0-9 digits)\n",
    "test = tf.constant(3)\n",
    "## architecture\n",
    "# input layer\n",
    "x = tf.placeholder(tf.float32, shape=[None] + list(n_input))\n",
    "# labels\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "# weights for conv layer 1 and 2 \n",
    "weights = {'convLayer 1': tf.Variable(tf.truncated_normal([5,5,1,6], stddev=0.1),name=\"convLayer1_weights\"), \n",
    "           'convLayer 2': tf.Variable(tf.truncated_normal([5,5,6,16], stddev=0.1),name=\"convLayer1_weights\")}\n",
    "bias = {'convLayer 1': tf.Variable(tf.zeros([6])), \n",
    "           'convLayer 2': tf.Variable(tf.zeros([16]))}\n",
    "conv1 = conv_layer(x, weights['convLayer 1'], bias=bias['convLayer 1'])\n",
    "pool1 = pooling_layer(conv1)\n",
    "conv2 = conv_layer(pool1, weights['convLayer 2'], bias['convLayer 2'], pad='VALID')\n",
    "pool2 = pooling_layer(conv2)\n",
    "flatten_layer = tf.contrib.layers.flatten(pool2)\n",
    "FC1 = tf.add(tf.matmul(flatten_layer, tf.Variable(tf.truncated_normal([400, 120], stddev=0.1))),\n",
    "             tf.Variable(tf.zeros([120])))\n",
    "FC1 = tf.nn.relu(FC1)\n",
    "FC2 = tf.add(tf.matmul(FC1, tf.Variable(tf.truncated_normal([120, 84], stddev=0.1))),\n",
    "             tf.Variable(tf.zeros([84])))\n",
    "FC2 = tf.nn.relu(FC2)\n",
    "logits = tf.add(tf.matmul(FC2, tf.Variable(tf.truncated_normal([84, 10], stddev=0.1))),\n",
    "                tf.Variable(tf.zeros([10])))\n",
    "\n",
    "# Loss function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "## ******************************************************************\n",
    "## Test model\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - cost= 0.12936906516551971, training_accuracy: 0.953125\n",
      "Epoch: 2 - cost= 0.0936274304986, training_accuracy: 0.9765625\n",
      "Epoch: 3 - cost= 0.052664242684841156, training_accuracy: 0.9765625\n",
      "Epoch: 4 - cost= 0.04286818578839302, training_accuracy: 0.9921875\n",
      "Epoch: 5 - cost= 0.02058977633714676, training_accuracy: 0.9921875\n",
      "Epoch: 6 - cost= 0.008332573808729649, training_accuracy: 1.0\n",
      "Epoch: 7 - cost= 0.0058052497915923595, training_accuracy: 1.0\n",
      "Epoch: 8 - cost= 0.006394656375050545, training_accuracy: 1.0\n",
      "Epoch: 9 - cost= 0.013510813936591148, training_accuracy: 0.9921875\n",
      "Epoch: 10 - cost= 0.010665135458111763, training_accuracy: 1.0\n",
      "Optimization Finished!\n",
      "Test Accuracy:0.988099992275238\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        total_batch = int(mnist_data.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist_data.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        # Display loss per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            c = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            train_score = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "            print(f\"Epoch: {epoch+1} - cost= {c}, training_accuracy: {train_score}\")\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = sess.run(accuracy, feed_dict={x: mnist_data.test.images, y: mnist_data.test.labels})\n",
    "    # Decrease test_size if you don't have enough memory\n",
    "    print(f\"Test Accuracy:{accuracy}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** as you can see the CNN manages to perform way better than a traditional fully connected Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
