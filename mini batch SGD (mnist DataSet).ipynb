{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-batch Stochastic Gradient Descent\n",
    "\n",
    "The aim of this notebook is to implement mini batch SGD on the mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot  as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /datasets/ud730/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting /datasets/ud730/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# reading mnist data\n",
    "mnist_data = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True, reshape=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set train and test data set\n",
    "train_data = mnist_data.train.images.astype(np.float32)\n",
    "test_data = mnist_data.test.images.astype(np.float32)\n",
    "train_labels = mnist_data.train.labels.astype(np.float32)\n",
    "test_labels = mnist_data.test.labels.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset size: (55000, 28, 28, 1)\n",
      "testing dataset size: (10000, 28, 28, 1)\n",
      "labels trainig dataset size: (55000, 10)\n",
      "labels testing dataset size: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "#Visualize shape of data\n",
    "print(f\"training dataset size: {train_data.shape}\")\n",
    "print(f\"testing dataset size: {test_data.shape}\")\n",
    "print(f\"labels trainig dataset size: {train_labels.shape}\")\n",
    "print(f\"labels testing dataset size: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(dataset, square_size, figureSize=5):\n",
    "    len_of_square = square_size\n",
    "    f, axarr = plt.subplots(len_of_square, len_of_square, figsize=(figureSize,figureSize))\n",
    "    j = 0\n",
    "    \n",
    "    for i in range(len_of_square*len_of_square):\n",
    "        j = j%len_of_square \n",
    "        if i%len_of_square==0 and i!=0:\n",
    "            j = j+1\n",
    "        axarr[j,i%len_of_square].imshow(dataset[i].reshape(28,28), cmap='gray')\n",
    "\n",
    "    plt.show();    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 25 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#training data\n",
    "visualize_data(train_data, 5, figureSize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaUAAAGdCAYAAACy+2xuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deZSU1Z3/8c9XRCRqHBoBiSIoLiOuICZIcEncADeMUVHGg9EjOgMORkPEZSaJcRudMcdRwWFGAjEOccGFqFEIByWLegQFBVEBjyBHFAjK4jIs3t8flP76e+2upWu7Vf1+nVOn+1P1VD3fpr9weerWcx8LIQgAgBRsV+0CAAD4EoMSACAZDEoAgGQwKAEAksGgBABIBoMSACAZRQ1KZjbQzN4ysyVmNrZURaG+0CfIB30CSVIIoUU3SW0kLZW0j6QdJM2X1CvHcwK3mr6tpk+40SfcSnFr7vdazJHStyUtCSG8E0LYJOl3ks4o4vWQvmUteA590vrQJ2ixYgalPSS91yivyNznmNkIM5tjZnOK2BdqF32CfNAnkCRtX8RzrYn7wtfuCGGCpAmSZGZfexx1jz5BPugTSCruSGmFpG6N8p6S3i+uHNQh+gT5oE8gqbhB6WVJ+5nZ3ma2g6ShkqaVpizUEfoE+aBPIKmIt+9CCFvMbJSkZ7XtkzMTQwgLS1YZ6gJ9gnzQJ/iSVfLSFbwHXPPmhhD6lnsn9EnNo0+QUwihqXlEVnQAAKSDQQkAkAwGJQBAMhiUAADJKObkWaBV+slPfuJy+/btXT700ENd/uEPf5j19caPH+/yCy+84PL9999faIlAzeJICQCQDAYlAEAyGJQAAMng5FkUolWeFPnggw+6nGuOqFhLly51+YQTTnB5+fLlZd1/CbTKPqm0/fff3+U333zT5dGjR7t81113lb2mQnDyLAAgeQxKAIBkMCgBAJLBeUpApNg5pPi9/WeffdblffbZx+XTTjvN5Z49e7o8bNgwl2+55ZaC6kF96t27t8tffPGFyytWrKhkOSXDkRIAIBkMSgCAZDAoAQCSwZwSWr2+ff0pNWeeeWbW7Rcu9BdEPf30011es2aNyxs3bnR5hx12cPnFF190+bDDDnO5Y8eOWetB63T44Ye7/Mknn7j82GOPVbKckuFICQCQDAYlAEAyGJQAAMmoqzml+HySSy65xOX333/f5c8//9zlBx54wOUPPvjA5SVLlhRbIhLUtWtXl838klzxHNLJJ5/s8sqVKwva31VXXeVyr169sm7/1FNPFfT6qE8HH3ywy6NGjXK5Xq67xZESACAZDEoAgGQwKAEAklFXc0q33Xabyz169Cjo+ZdeeqnLGzZscDmeW6i0eC2r+OedM2dOJcupG7///e9d3nfffV2O+2Dt2rVF7W/o0KEut23btqjXQ+vw93//9y7vtNNOLsdrNtYqjpQAAMlgUAIAJINBCQCQjLqaU4rPSzr00ENdXrRokcsHHnigy3369HH5uOOOc7lfv34uv/feey5369Yt71olacuWLS6vXr3a5fj8mdjy5ctdZk6pNJYtW1bS1xszZozL+++/f9btX3rppawZrdNPf/pTl+M+rZe//xwpAQCSwaAEAEhGzkHJzCaa2SozW9DovgYzm2FmizNfO5S3TKSOPkE+6BPkYiGE7BuYHSNpo6TfhBAOztx3m6S1IYRbzWyspA4hhKtz7sws+84S06GD/7sRX79k7ty5Lh955JEFvX689t7bb7/tcjwH1tDQ4PLIkSNdHj9+fEH7b4G5IYS+TT3Qmvskduqpp7r88MMPuxxfT2nVqlUux+cxPf/88yWsriLokxKIz7N85513XI7/vYjPY0pdCMGauj/nkVIIYbak+GzBMyRNznw/WdKQoqpDzaNPkA/6BLm09NN3XUIIKyUphLDSzDo3t6GZjZA0ooX7QW2jT5AP+gRfKftHwkMIEyRNkOr/cBstR58gH/RJ/WvpoPShmXXN/K+mq6RVOZ9Rgz766COXZ82alXX7mTNnFrW/s846y+V4Tuv11193uQbWumoVfRLr29dPp8RzSLH491iDc0jFapV9ksuxxx6b9fH4vMZ60dKPhE+TNDzz/XBJT5SmHNQZ+gT5oE/wlXw+Ej5F0guSDjCzFWZ2saRbJZ1oZoslnZjJaMXoE+SDPkEuOd++CyGc18xDx5e4FtQw+gT5oE+QS12tfVdrOnf2HzIaN26cy9tt5w9kb7jhBpeLva4PSuPxxx93+aSTTsq6/W9+8xuXr7/++pLXhNp3yCGHZH08vp5avWCZIQBAMhiUAADJYFACACSDOaUqiteu69Spk8vxeVJvvfVW2WtCbvF1rvr37+9yu3btXF6zZo3LN954o8sbN24sYXWoVfH12n70ox+5/Oqrr7o8Y8aMstdUDRwpAQCSwaAEAEgGgxIAIBnMKVXQd7/7XZfHjh2bdfshQ/wK/gsWLGhmS1TS1KlTXe7YsWPW7X/729+6vHTp0pLXhNp3wgknuBxfP+2ZZ55xOb4eW73gSAkAkAwGJQBAMhiUAADJYE6pggYPHuxy27ZtXY6vx/TCCy+UvSbkdvrpp7vcp0+frNs/99xzLv/sZz8rdUmoQ4cddpjLIfhrGD7yyCOVLKdqOFICACSDQQkAkAwGJQBAMphTKqP27du7PHDgQJc3bdrkcjz3sHnz5vIUhqzi846uvfZal+O5wNi8efNcZm07NGX33Xd3+eijj3Y5XuvyscceK3tNKeBICQCQDAYlAEAyGJQAAMlgTqmMxowZ43Lv3r1djtey+utf/1r2mpDbVVdd5fKRRx6ZdfvHH3/cZc5LQj4uvPBClzt37uzyH/7whwpWkw6OlAAAyWBQAgAkg0EJAJAM5pRK6JRTTnH5X/7lX1xev369yzfccEPZa0LhrrzyyoK2HzVqlMucl4R8dO/ePevjH330UYUqSQtHSgCAZDAoAQCSwaAEAEgGc0pFiNdI+8///E+X27Rp4/LTTz/t8osvvliewlBRDQ0NLhe7ZuG6deuyvl689t6uu+6a9fX+7u/+zuVC58y2bt3q8tVXX+3yp59+WtDrYZtTTz016+O///3vK1RJWjhSAgAkg0EJAJCMnIOSmXUzs1lmtsjMFprZ6Mz9DWY2w8wWZ752KH+5SBV9gnzQJ8glnzmlLZKuCiG8Yma7SJprZjMkXShpZgjhVjMbK2mspKuzvE7Ni+eI4rXr9t57b5eXLl3qcnzeUp1ptX3y2muvlfT1Hn74YZdXrlzpcpcuXVw+99xzS7r/XD744AOXb7rppkKe3mr7ZMCAAS7H11PCNjmPlEIIK0MIr2S+3yBpkaQ9JJ0haXJms8mShpSrSKSPPkE+6BPkUtCn78ysh6Tekl6S1CWEsFLa1mhm1rmZ54yQNKK4MlFL6BPkgz5BU/IelMxsZ0lTJV0RQlhvZnk9L4QwQdKEzGuElhSJ2kGfIB/0CZqT16BkZm21rYEeCCE8mrn7QzPrmvlfTVdJq8pVZCp69uzp8hFHHJF1+/h8kHiOqd7US5/E55OdccYZFd3/2WefXdTzt2zZ4vIXX3yRdftp06a5PGfOnKzb/+lPf2pZYRn10ieFOvPMM12O56hfffVVl2fPnl32mlKUz6fvTNJ9khaFEO5o9NA0ScMz3w+X9ETpy0OtoE+QD/oEueRzpPRdSRdIet3M5mXuu1bSrZIeMrOLJS2XVNx/71Dr6BPkgz5BVjkHpRDCnyU194bv8aUtB7WKPkE+6BPkwtp3WcTXO5k+fXrW7ceMGePyk08+WfKaUH4/+MEPXP7pT3/qcrz2XC4HHXSQy4WeVzRx4kSX33333azbT5061eU333yzoP2hNL7xjW+4PHjw4KzbP/LIIy7Haw62FiwzBABIBoMSACAZDEoAgGQwp5TFiBH+xPG99tor6/bPP/+8yyFwbl89uO2220r6eueff35JXw9piq+D9dFHH7kcnx925513lr2mWsCREgAgGQxKAIBkMCgBAJLBnFIj8fVOLr/88ipVAqDWxXNK/fv3r1IltYUjJQBAMhiUAADJYFACACSDOaVGjj76aJd33nnnrNvH10fauHFjyWsCgNaEIyUAQDIYlAAAyWBQAgAkgzmlAsyfP9/l44/31yRbu3ZtJcsBgLrDkRIAIBkMSgCAZDAoAQCSYZW85o+ZcYGh2jY3hNC33DuhT2oefYKcQgjW1P0cKQEAksGgBABIBoMSACAZlT5PaY2kZZJ2y3yfKuprWvcK7Yc+KQ36JA0p15dcj1T0gw5f7dRsTiUmQluK+tKQ+s9JfWlI/edMub4Ua+PtOwBAMhiUAADJqNagNKFK+80X9aUh9Z+T+tKQ+s+Zcn3J1VaVOSUAAJrC23cAgGQwKAEAklHRQcnMBprZW2a2xMzGVnLfzTGziWa2yswWNLqvwcxmmNnizNcOVaqtm5nNMrNFZrbQzEanVF+5pNYnKfdIphb6hD7Jp76a6JOKDUpm1kbSPZIGSeol6Twz61Wp/WcxSdLA6L6xkmaGEPaTNDOTq2GLpKtCCAdK6idpZObPLJX6Si7RPpmkdHtEok/ok/zURp+EECpyk3SUpGcb5WskXVOp/eeorYekBY3yW5K6Zr7vKumtateYqeUJSSemWl8990mt9Ah9UvW66JMib5V8+24PSe81yisy96WoSwhhpSRlvnaucj0ysx6Sekt6SQnWV0K10idJ/g7ok+Qk+TtIuU8qOSg1de0MPo+eBzPbWdJUSVeEENZXu54yo09aiD6hT/KRep9UclBaIalbo7ynpPcruP9CfGhmXSUp83VVtQoxs7ba1kAPhBAeTa2+MqiVPknqd0Cf0Cf5qIU+qeSg9LKk/cxsbzPbQdJQSdMquP9CTJM0PPP9cG1777XizMwk3SdpUQjhjkYPJVFfmdRKnyTzO6BP6JN81EyfVHhibbCktyUtlXRdtSf6MjVNkbRS0mZt+9/XxZI6atunUBZnvjZUqbYB2vaWxGuS5mVug1Opr7X0Sco9Qp/QJ/XWJywzBABIBis6AACSwaAEAEgGgxIAIBkMSgCAZDAoAQCSUdSglNoqvUgTfYJ80CeQ1PLzlCS10bbzA/aRtIOk+ZJ65XhO4FbTt9X0CTf6hFspbs39Xos5Uvq2pCUhhHdCCJsk/U7SGUW8HtK3rAXPoU9aH/oELVbMoJTXKr1mNsLM5pjZnCL2hdpFnyAf9AkkSdsX8dy8VukNIUyQNEGSzOxrj6Pu0SfIB30CScUdKdXKKr2oLvoE+aBPIKm4QalWVulFddEnyAd9AklFvH0XQthiZqMkPattn5yZGEJYWLLKUBfoE+SDPsGXKrpKOO8B17y5IYS+5d4JfVLz6BPkFEJoah6RFR0AAOlgUAIAJINBCQCQDAYlAEAyGJQAAMlgUAIAJINBCQCQjGLWvmt1dtppJ5dvv/12ly+99FKX586d6/LZZ5/t8rJlLVlMGQDqF0dKAIBkMCgBAJLBoAQASAZzSgXo2rWry5dcconLX3zxhctHHHGEy6eeeqrL99xzTwmrQ6X06dPH5UcffdTlHj16VLAa6aSTTnJ50aJFLr/33ntC/TnttNNcnjbNL6o+atQol++9916Xt27dWp7CisSREgAgGQxKAIBkMCgBAJLBnFIWnTp1cnny5MlVqgQpOfnkk11u165dlSrZJp5buOiii1weOnRoJctBmXTs2NHlcePGZd3+7rvvdnnixIkuf/bZZ6UprMQ4UgIAJINBCQCQDAYlAEAymFNq5J//+Z9dHjJkiMvf/va3i3r9Y445xuXttvP/J5g/f77Ls2fPLmp/KI3tt/d/TQYPHlylSpoWr7F45ZVXuhyv2fjJJ5+UvSaUXvzvx5577pl1+ylTprj8+eefl7ymcuBICQCQDAYlAEAyGJQAAMlgTqmRX/3qVy7Ha9kV6wc/+EHWHF9f6dxzz3U5njtAZXzve99z+aijjnL5tttuq2Q5X9OhQweXe/Xq5fI3vvENl5lTqg3x+W/XXXddQc+///77XQ4hFF1TJXCkBABIBoMSACAZDEoAgGRYJd9nNLOk3tR8+umnXR40aJDLxc4p/e1vf3N548aNLnfv3r2g12vTpk1R9ZTA3BBC33LvpNp9cvDBB7v83HPPuRz/XuPrZsW/53KL6xswYIDL8XXAVq9eXe6SWkWflFvfvv6P8OWXX866/ZYtW1xu27ZtyWsqpRCCNXU/R0oAgGQwKAEAkpFzUDKziWa2yswWNLqvwcxmmNnizNcO2V4D9Y8+QT7oE+SSz3lKkyTdLek3je4bK2lmCOFWMxubyVeXvrzSOvbYY10+4IADXI7nkAqdU7r33ntdnj59usvr1q1z+fvf/77Luc5D+Md//EeXx48fX1B9ZTZJddIn119/vcvx2nEDBw50udJzSA0NDS7HfV3q8+tKbJLqpE/K7ayzzipo+/jfm1qV80gphDBb0tro7jMkfXnFu8mShgitGn2CfNAnyKWlc0pdQggrJSnztXPpSkIdoU+QD/oEXyn7MkNmNkLSiHLvB7WNPkE+6JP619JB6UMz6xpCWGlmXSWtam7DEMIESROkyp9X0KNHD5d/97vfubzbbrsV9Hrx2nRTp051+Re/+IXLn376aUGvN2KE/7vWqVMnl+M11nbccUeX7777bpc3b96cdf8VUBN98sMf/tDl+HpJS5YscXnOnDllrymbeO4xnkOKz1v6+OOPy11SsWqiTyotvn5SbNOmTS4XujZeqlr69t00ScMz3w+X9ERpykGdoU+QD/oEX8nnI+FTJL0g6QAzW2FmF0u6VdKJZrZY0omZjFaMPkE+6BPkkvPtuxDCec08dHyJa0ENo0+QD/oEudT19ZS2397/eIXOIT3//PMuDx061OU1a9a0rLCMeE7plltucfmOO+5wOb4uTjzHNG3aNJeXLl1aVH2txdlnn+1y/Oc8bty4SpbzNfHc6LBhw1zeunWryzfeeKPLCcwtIg/9+/fPmmPxdbHmzZtX8pqqgWWGAADJYFACACSDQQkAkIy6nlMqVHz+yUUXXeRysXNIucRzQvHcwZFHHlnW/bcWu+66q8v9+vXLun211xiMz1+L50YXLVrk8qxZs8peE0qv0L/f1e7LcuFICQCQDAYlAEAyGJQAAMloVXNK222XfQz+zne+U6FKmmbmL1kf15ur/p///OcuX3DBBSWpq960a9fO5T322MPlKVOmVLKcnHr27Jn18QULFmR9HLWhb9++WR+P1zBkTgkAgDJjUAIAJINBCQCQjLqeU7rssstcjq87k5rTTjvN5d69e7sc1x/neE4JTduwYYPL8Zphhx56qMsNDQ0ur10bX827tDp39hdeja/3FPvzn/9cznJQJgMGDHD5/PPPz7r9unXrXF6xYkXJa0oBR0oAgGQwKAEAksGgBABIRl3PKcVzNNXWqVMnl3v16uXytddeW9DrrV692mWum5Ofzz77zOX4ulNnnXWWy0899ZTL8XWuCnXwwQe7vM8++7gcXz8phJD19VKfK0XTOnbs6HKu8xBnzJhRznKSwZESACAZDEoAgGQwKAEAklHXc0qpue6661weOXJkQc9/9913XR4+fLjLy5cvb1Fdrd3PfvYzl+M1CE855RSXi10bL74uVzxnFF8vKZdJkyYVVQ+qI9f5Z/Fad//1X/9VznKSwZESACAZDEoAgGQwKAEAksGcUhk9/fTTLh9wwAFFvd4bb7zhMmuelcabb77p8jnnnOPy4Ycf7vK+++5b1P4eeeSRrI9PnjzZ5WHDhmXdPj7vCmnac889Xc611l28tt2cOXNKXlOKOFICACSDQQkAkAwGJQBAMup6Tik+3yTX2lKDBg3K+viECRNc/ta3vpV1+3h/xa5Rltpafq1FfL2lOJfaO++8U9D28Vp6CxYsKGU5KJH+/fu7nOvfo8cff7yc5SSLIyUAQDIYlAAAycg5KJlZNzObZWaLzGyhmY3O3N9gZjPMbHHma4fyl4tU0SfIB32CXPKZU9oi6aoQwitmtoukuWY2Q9KFkmaGEG41s7GSxkq6unylFm78+PEu33bbbVm3f/LJJ13ONQdU6BxRodvfe++9BW1fZTXbJ6mJ50LjHKuxOaRW2yfx9ZNi8ZqId955ZznLSVbOI6UQwsoQwiuZ7zdIWiRpD0lnSPryLL/JkoaUq0ikjz5BPugT5FLQnJKZ9ZDUW9JLkrqEEFZK2xpNUudSF4faRJ8gH/QJmpL3R8LNbGdJUyVdEUJYn+sthUbPGyFpRMvKQ62hT5AP+gTNyWtQMrO22tZAD4QQHs3c/aGZdQ0hrDSzrpJWNfXcEMIESRMyrxOa2qZcHn30UZfHjBnjcqdOnSpZjlavXu3yokWLXB4xwv9dW7lyZdlrKqVa7ZPUxNdXinOta619cvLJJ2d9PL4e2rp168pZTrLy+fSdSbpP0qIQwh2NHpom6curzA2X9ETpy0OtoE+QD/oEueRzpPRdSRdIet3MvjyV/VpJt0p6yMwulrRc0tnlKRE1gj5BPugTZJVzUAoh/FlSc2/4Hl/aclCr6BPkgz5BLnW99t2yZctcHjp0qMtDhvhPnY4ePbqs9dx0000u33PPPWXdH2rTjjvumPVxrp9UG9q2betyz549s27/+eefu7x58+aS11QLWGYIAJAMBiUAQDIYlAAAyajrOaXY7Nmzs+bp06e7HJ83FF/PaNq0aS7H11uKTwh844038i8WrdaPfvQjlz/++GOXf/nLX1ayHLRQvNblnDlzXI6vg7VkyZKy11QLOFICACSDQQkAkAwGJQBAMlrVnFIuzzzzTNYMVMLLL7/s8h133OHyrFmzKlkOWmjr1q0uX3fddS7HaxrOnTu37DXVAo6UAADJYFACACSDQQkAkAyr5LVaau36J/iauSGEvuXeCX1S8+gT5BRCaHJhXo6UAADJYFACACSDQQkAkAwGJQBAMhiUAADJYFACACSDQQkAkAwGJQBAMhiUAADJYFACACSDQQkAkIxKX09pjaRlknbLfJ8q6mta9wrthz4pDfokDSnXl1yPVHRB1q92ajanEgs2thT1pSH1n5P60pD6z5lyfSnWxtt3AIBkMCgBAJJRrUFpQpX2my/qS0PqPyf1pSH1nzPl+pKrrSpzSgAANIW37wAAyWBQAgAko6KDkpkNNLO3zGyJmY2t5L6bY2YTzWyVmS1odF+Dmc0ws8WZrx2qVFs3M5tlZovMbKGZjU6pvnJJrU9S7pFMLfQJfZJPfTXRJxUblMysjaR7JA2S1EvSeWbWq1L7z2KSpIHRfWMlzQwh7CdpZiZXwxZJV4UQDpTUT9LIzJ9ZKvWVXKJ9Mknp9ohEn9An+amNPgkhVOQm6ShJzzbK10i6plL7z1FbD0kLGuW3JHXNfN9V0lvVrjFTyxOSTky1vnruk1rpEfqk6nXRJ0XeKvn23R6S3muUV2TuS1GXEMJKScp87VzlemRmPST1lvSSEqyvhGqlT5L8HdAnyUnyd5Byn1RyULIm7uPz6Hkws50lTZV0RQhhfbXrKTP6pIXoE/okH6n3SSUHpRWSujXKe0p6v4L7L8SHZtZVkjJfV1WrEDNrq20N9EAI4dHU6iuDWumTpH4H9Al9ko9a6JNKDkovS9rPzPY2sx0kDZU0rYL7L8Q0ScMz3w/XtvdeK87MTNJ9khaFEO5o9FAS9ZVJrfRJMr8D+oQ+yUfN9EmFJ9YGS3pb0lJJ11V7oi9T0xRJKyVt1rb/fV0sqaO2fQplceZrQ5VqG6Btb0m8Jmle5jY4lfpaS5+k3CP0CX1Sb33CMkMAgGSwogMAIBkMSgCAZDAoAQCSwaAEAEgGgxIAIBlFDUqprdKLNNEnyAd9AkktP09JUhttOz9gH0k7SJovqVeO5wRuNX1bTZ9wo0+4leLW3O+1mCOlb0taEkJ4J4SwSdLvJJ1RxOshfcta8Bz6pPWhT9BixQxKea3Sa2YjzGyOmc0pYl+oXfQJ8kGfQJK0fRHPzWuV3hDCBEkTJMnMvvY46h59gnzQJ5BU3JFSrazSi+qiT5AP+gSSihuUamWVXlQXfYJ80CeQVMTbdyGELWY2StKz2vbJmYkhhIUlqwx1gT5BPugTfKmiq4TzHnDNmxtC6FvundAnNY8+QU4hhKbmEVnRAQCQDgYlAEAyGJQAAMlgUAIAJINBCQCQDAYlAEAyGJQAAMlgUAIAJKOYBVkBAIno0KGDy3vttVdBz1+2zF9x5Mc//rHLCxYscPntt992ef78+QXtrzkcKQEAksGgBABIBoMSACAZrXpOqXPnzi4/9NBDLv/1r391ecKECS6/++67ZakrX7vuuqvLxxxzjMvPPPOMy5s3by57TQDK45RTTnH59NNPd/m4445zed999y3o9eM5ou7du7vcrl27rM9v06ZNQftrDkdKAIBkMCgBAJLBoAQASEarmlOKP8e/cKG/sGU8R/Phhx+6nNoc0ty5c13u1KmTy0cccYTLS5YsKU9hrdw3v/lNl2+55RaXDz74YJdPOOEEl5nra5169uzp8siRI12+5JJLXG7fvr3LZk1eI6/F9t9//5K+XktxpAQASAaDEgAgGQxKAIBk1PWc0m677ebygw8+6HJDQ4PL48aNc/nyyy8vT2EtdP3117u89957u3zppZe6zBxSeQwbNszlm266yeVu3bplfX48B/W3v/2tNIWhpuy5554ujx49uqL7f/PNN12O59irhSMlAEAyGJQAAMlgUAIAJMNCCJXbmVnldibppJNOcvkPf/hD1u133313l1evXl3ymgpx0EEHufz666+7/Nhjj7l84YUXurxhw4ZSlzQ3hNC31C8aq3Sf5BK/9//qq6+63LFjR5dz/Z2K5zZHjRrl8tq1awstMTWtok/iOet4Tugvf/mLy/FalP369XP56aefdvmTTz5xeaeddnJ5+vTpLsfXO3rppZdcjvv2s88+y7q/cgshNHmiFUdKAIBkMCgBAJLBoAQASEZdnacUXx/prLPOyrr9xRdf7HJqc0h//OMfs24fzymVYQ4Jkn7yk5+4HJ/fVqhzzz3X5YEDB7ocn/d01113ubxp06ai9o+WyTWnc9hhh7l85plnZn29F1980eU+ffq4HK+1uddee7m8YqCIXLoAAAwMSURBVMUKl7/44ous+6sVHCkBAJLBoAQASEbOQcnMJprZKjNb0Oi+BjObYWaLM187ZHsN1D/6BPmgT5BLzvOUzOwYSRsl/SaEcHDmvtskrQ0h3GpmYyV1CCFcnXNnZT6v4P7773f5H/7hH1yOrz907LHHulzpz+nHLrvsMpfjtfgmTZrk8kUXXVTukmLNnn9SS32SS/fu3V1+7bXXXN55551djs8fi6/DFV8/KZdVq1a53Lt3b5c/+OCDgl6vCuqiT3bYYQeXH374YZdPPfVUl2+++WaX4+tqffrppyWsrva1+DylEMJsSfHZfGdImpz5frKkIUVVh5pHnyAf9Alyaemn77qEEFZKUghhpZl1bm5DMxshaUQL94PaRp8gH/QJvlL2j4SHECZImiBV/20ZpIs+QT7ok/rX0kHpQzPrmvlfTVdJq3I+owLi+bH4c/vvv/++y5U+36N9+/YuX3vttS7/0z/9k8vxz1OFOaRiJdknuRx++OEu77LLLi7/6U9/cjmem9xxxx1dPu+881yOf+89e/Z0OV6D8YknnnB50KBBLtfBWnlJ9Ek8V3jNNde4HM8hrVmzxuV///d/d5k5pJZp6UfCp0kanvl+uKQnsmyL1os+QT7oE3wln4+ET5H0gqQDzGyFmV0s6VZJJ5rZYkknZjJaMfoE+aBPkEvOt+9CCOc189DxJa4FNYw+QT7oE+RSV2vf5XLKKae4HK9d9fHHH7s8fvz4ovYXzzUcd9xxLsfXU4k98sgjRe0fLdOuXTuX47m9X/3qV1mf//nnn7v861//2uWzzz7b5X322Sfr68VzE6x9Vx5DhvhPoo8dO9bl5cuXu3z00Ue7vG7duvIU1sqwzBAAIBkMSgCAZDAoAQCSUVdzSnfeeafL3/ve91z+1re+5fIxxxzjsplfiun0008vqp749XKtM/jOO++4HJ/PgsqIzyuKxXOTjz/+eEGv37dvk8vCNSu+7s7GjRsLej7y079//6yPv/rqqy7H1zNCaXCkBABIBoMSACAZDEoAgGTkvJ5SSXdW4QUUO3Tw1wqL1zQbOHCgy2PGjHE5vq7N5MmTVYj4+k7z58/Puv1vf/tbl4cPH97MllXT7HVySqnaC22ec845Lk+ZMsXl+PpJQ4cOdfmQQw5x+cwzz3Q5Pk9p/fr1Lsd9G69tF8+FvvHGG0pMTfZJ/Pe9Y8eOLv/f//2fy//2b//mcrxG4bx580pYXf1p8fWUAACoFAYlAEAyGJQAAMmo6zmlaovXNFuyZInL8XvOJ598ssurV68uT2EtV5NzBYVqaGhwOf697brrri4Xej7aH//4R5dHjhzp8pNPPunyfvvt5/J///d/u3zZZZdl3V8V1GSf5LoeWy7x9vfee6/L8flme+21l8txny1cuDDr/g466CCXX3jhBZdTP4+KOSUAQPIYlAAAyWBQAgAkgzmlMpo0aZLLF1xwgcvxeVIzZswod0nFqsm5gmKdcMIJLsfXuYrnmOK/U3fddZfLV199tcvx9Zduvvlml+Pr+ixbtixrfUuXLlWV1WSf3H777S5feeWVpXz5sovnoJ977jmX4/Ppqo05JQBA8hiUAADJYFACACSDOaUSitc0e/DBB13esGGDy/H1nl555ZXyFFY6NTlXUGrxHM7555/v8scff+zyv/7rv7qc63pI7du3d/l///d/XY6v85Xgmok12Sdt2rRxuXfv3i7Hv4ftt/eXo+vWrZvL221X3f/zx/+2//znP3f5xhtvrGA1X8ecEgAgeQxKAIBkMCgBAJKxfe5NkK9BgwZlfTxe06wG5pDQhHjtujgX67PPPnM5npuM55Tiucl47b74ekxo2tatW12eM2eOy/vvv3/W5x9//PEut23b1uV4TufII48ssMLCxGsyHnHEEWXdX6lwpAQASAaDEgAgGQxKAIBkMKdUQvGc0ieffOLyf/zHf1SyHNSJhx56yOV4Tuncc891edSoUS7fcMMN5SkMzsyZM7M+fvjhh7sczylt2bLF5V//+tcux9fRuuKKK1yOz5erVRwpAQCSwaAEAEhGzkHJzLqZ2SwzW2RmC81sdOb+BjObYWaLM187lL9cpIo+QT7oE+SSc+07M+sqqWsI4RUz20XSXElDJF0oaW0I4VYzGyupQwjh6iwvlfyaZoW67LLLXB43bpzLq1atcnn33Xcve01l1uyaZvRJ5cRzE3/5y19c3nHHHV0+8MADXX777bfLU9j/R580oU+fPi6//PLLBT1/1qxZLh933HEux+clxeJ/ny6//PKC9l9qLV77LoSwMoTwSub7DZIWSdpD0hmSJmc2m6xtjYVWij5BPugT5FLQp+/MrIek3pJektQlhLBS2tZoZta5meeMkDSiuDJRS+gT5IM+QVPyHpTMbGdJUyVdEUJYn+tQ8UshhAmSJmReo6YOt1E4+gT5oE/QnLwGJTNrq20N9EAI4dHM3R+aWdfM/2q6SlrV/CvUp3hOKZ6fe+qpp7I+f5dddnG5Qwc/t7t8+fIiqqs8+qQy5s2b53J8vabbb7/d5ZtvvtnlCy64wOV4rb1ya619smjRIpfj88/OOeecrM+P1ziMxWv3xf/+jB07NleJScjn03cm6T5Ji0IIdzR6aJqkL68mNlzSE6UvD7WCPkE+6BPkks+R0nclXSDpdTP78r9o10q6VdJDZnaxpOWSzm7m+Wgd6BPkgz5BVjkHpRDCnyU194bv8c3cj1aGPkE+6BPkkvM8pZLurM4mJuP39g855BCX77vvPpeff/55l3/84x+7vHDhQpeHDx+uxDR7/kkp1VuflFunTp1cjs9b2nfffV2Oz3N67bXXSl0SfZKHLl26uPw///M/Lvft6/8IO3f2H0h89913Xb7//vtdjq/flJoWn6cEAEClMCgBAJLBoAQASAZzSkXINacUnxAY/1nHc06//OUvXX7vvfeKLbHUmCuoAXvttZfL8dzDlClTXB42bFipS6BPSiA+n6xfv34u/+IXv3A5XmszdcwpAQCSx6AEAEgGgxIAIBnMKRVhwIABLt9www0uz5492+Xx48e7/NFHH7m8adOmElZXFswV1KDp06e7fNRRR7n8ne98x+U33nij2F3SJ8iJOSUAQPIYlAAAyWBQAgAkgzklFIK5ghr0zW9+0+X58+e7PHr0aJenTZtW7C7pE+TEnBIAIHkMSgCAZDAoAQCSkc+VZwHUsPXr17u89957V6kSIDeOlAAAyWBQAgAkg0EJAJAMBiUAQDIYlAAAyWBQAgAkg0EJAJCMSp+ntEbSMkm7Zb5PFfU1rXuF9kOflAZ9koaU60uuRyq6IOtXOzWbU4kFG1uK+tKQ+s9JfWlI/edMub4Ua+PtOwBAMhiUAADJqNagNKFK+80X9aUh9Z+T+tKQ+s+Zcn3J1VaVOSUAAJrC23cAgGQwKAEAklHRQcnMBprZW2a2xMzGVnLfzTGziWa2yswWNLqvwcxmmNnizNcOVaqtm5nNMrNFZrbQzEanVF+5pNYnKfdIphb6hD7Jp76a6JOKDUpm1kbSPZIGSeol6Twz61Wp/WcxSdLA6L6xkmaGEPaTNDOTq2GLpKtCCAdK6idpZObPLJX6Si7RPpmkdHtEok/ok/zURp+EECpyk3SUpGcb5WskXVOp/eeorYekBY3yW5K6Zr7vKumtateYqeUJSSemWl8990mt9Ah9UvW66JMib5V8+24PSe81yisy96WoSwhhpSRlvnaucj0ysx6Sekt6SQnWV0K10idJ/g7ok+Qk+TtIuU8qOShZE/fxefQ8mNnOkqZKuiKEsL7a9ZQZfdJC9Al9ko/U+6SSg9IKSd0a5T0lvV/B/RfiQzPrKkmZr6uqVYiZtdW2BnoghPBoavWVQa30SVK/A/qEPslHLfRJJQellyXtZ2Z7m9kOkoZKmlbB/RdimqThme+Ha9t7rxVnZibpPkmLQgh3NHooifrKpFb6JJnfAX1Cn+SjZvqkwhNrgyW9LWmppOuqPdGXqWmKpJWSNmvb/74ultRR2z6FsjjztaFKtQ3QtrckXpM0L3MbnEp9raVPUu4R+oQ+qbc+YZkhAEAyWNEBAJAMBiUAQDIYlAAAyWBQAgAkg0EJAJAMBiUAQDIYlAAAyfh/D7qp1PCRCMcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 504x504 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#testing data\n",
    "visualize_data(test_data,3,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get batches function \n",
    "This function will help us getting the batches for performing SGD\n",
    "* Note: we don't need to shuffle the batches because mnist data already comes shuffled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    # TODO: Implement batching\n",
    "    output_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        output_batches.append(batch)\n",
    "        \n",
    "    return output_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor flow section\n",
    "\n",
    "### Setting-up the architecture of the nn which will have the below form:\n",
    "\n",
    "![](figures/figure2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_shape = train_data.reshape(-1,784)\n",
    "test_data_shape = test_data.reshape(-1,784)\n",
    "no_features = train_data_shape.shape[1]\n",
    "no_classes = train_labels.shape[1]\n",
    "\n",
    "## tensors definition+\n",
    "learning_rate_placeholder = tf.placeholder(tf.float32)\n",
    "features = tf.placeholder(dtype=tf.float32, shape=[None, no_features])\n",
    "labels = tf.placeholder(dtype=tf.float32, shape=[None, no_classes])\n",
    "weights = tf.Variable(tf.truncated_normal([no_features, no_classes]),dtype=tf.float32)\n",
    "bias = tf.Variable(tf.truncated_normal([no_classes]),dtype=tf.float32)\n",
    "logits = tf.add(tf.matmul(features, weights), bias)\n",
    "# cost function crossentropy\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "# optimizer set as gradien descent, but as we want minibatch SGD we will feed the data randomly and batches\n",
    "learning_rate_placeholder = tf.placeholder(tf.float32)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate_placeholder).minimize(cost)\n",
    "# Calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Tensorflow session and train the nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training epoch: 0 - loss: 10.84433650970459, accuracy: 0.11363636702299118\n",
      " Training epoch: 1 - loss: 9.982397079467773, accuracy: 0.14772726595401764\n",
      " Training epoch: 2 - loss: 9.334877014160156, accuracy: 0.1818181872367859\n",
      " Training epoch: 3 - loss: 8.812315940856934, accuracy: 0.1818181872367859\n",
      " Training epoch: 4 - loss: 8.354653358459473, accuracy: 0.20454545319080353\n",
      " Training epoch: 5 - loss: 7.940855503082275, accuracy: 0.22727273404598236\n",
      " Training epoch: 6 - loss: 7.561100482940674, accuracy: 0.23863635957241058\n",
      " Training epoch: 7 - loss: 7.208675861358643, accuracy: 0.2613636255264282\n",
      " Training epoch: 8 - loss: 6.88070821762085, accuracy: 0.27272728085517883\n",
      " Training epoch: 9 - loss: 6.575801372528076, accuracy: 0.27272728085517883\n",
      " Training epoch: 10 - loss: 6.2927327156066895, accuracy: 0.27272728085517883\n",
      " Training epoch: 11 - loss: 6.030267238616943, accuracy: 0.28409090638160706\n",
      " Training epoch: 12 - loss: 5.787132740020752, accuracy: 0.28409090638160706\n",
      " Training epoch: 13 - loss: 5.561960220336914, accuracy: 0.2954545319080353\n",
      " Training epoch: 14 - loss: 5.3533244132995605, accuracy: 0.3181818127632141\n",
      " Training epoch: 15 - loss: 5.159847736358643, accuracy: 0.3295454680919647\n",
      " Training epoch: 16 - loss: 4.980295181274414, accuracy: 0.3295454680919647\n",
      " Training epoch: 17 - loss: 4.8136162757873535, accuracy: 0.34090909361839294\n",
      " Training epoch: 18 - loss: 4.65890645980835, accuracy: 0.375\n",
      " Training epoch: 19 - loss: 4.515346050262451, accuracy: 0.3863636255264282\n",
      " Training epoch: 20 - loss: 4.382158279418945, accuracy: 0.39772728085517883\n",
      " Training epoch: 21 - loss: 4.258605480194092, accuracy: 0.39772728085517883\n",
      " Training epoch: 22 - loss: 4.143977642059326, accuracy: 0.39772728085517883\n",
      " Training epoch: 23 - loss: 4.037578582763672, accuracy: 0.4204545319080353\n",
      " Training epoch: 24 - loss: 3.9387013912200928, accuracy: 0.4204545319080353\n",
      " Training epoch: 25 - loss: 3.8466200828552246, accuracy: 0.4204545319080353\n",
      " Training epoch: 26 - loss: 3.760622978210449, accuracy: 0.4431818127632141\n",
      " Training epoch: 27 - loss: 3.680026054382324, accuracy: 0.46590909361839294\n",
      " Training epoch: 28 - loss: 3.6042070388793945, accuracy: 0.46590909361839294\n",
      " Training epoch: 29 - loss: 3.5326125621795654, accuracy: 0.47727271914482117\n",
      " Training epoch: 30 - loss: 3.464763641357422, accuracy: 0.47727271914482117\n",
      " Training epoch: 31 - loss: 3.4002561569213867, accuracy: 0.4886363744735718\n",
      " Training epoch: 32 - loss: 3.338745355606079, accuracy: 0.47727271914482117\n",
      " Training epoch: 33 - loss: 3.2799508571624756, accuracy: 0.47727271914482117\n",
      " Training epoch: 34 - loss: 3.223633050918579, accuracy: 0.4886363744735718\n",
      " Training epoch: 35 - loss: 3.169593095779419, accuracy: 0.4886363744735718\n",
      " Training epoch: 36 - loss: 3.1176626682281494, accuracy: 0.4886363744735718\n",
      " Training epoch: 37 - loss: 3.0676963329315186, accuracy: 0.4886363744735718\n",
      " Training epoch: 38 - loss: 3.0195696353912354, accuracy: 0.4886363744735718\n",
      " Training epoch: 39 - loss: 2.9731740951538086, accuracy: 0.4886363744735718\n",
      " Training epoch: 40 - loss: 2.9284141063690186, accuracy: 0.5\n",
      " Training epoch: 41 - loss: 2.885199785232544, accuracy: 0.5\n",
      " Training epoch: 42 - loss: 2.843456983566284, accuracy: 0.5\n",
      " Training epoch: 43 - loss: 2.8031113147735596, accuracy: 0.5227272510528564\n",
      " Training epoch: 44 - loss: 2.7640998363494873, accuracy: 0.5227272510528564\n",
      " Training epoch: 45 - loss: 2.726363182067871, accuracy: 0.5227272510528564\n",
      " Training epoch: 46 - loss: 2.6898488998413086, accuracy: 0.5227272510528564\n",
      " Training epoch: 47 - loss: 2.6545064449310303, accuracy: 0.5227272510528564\n",
      " Training epoch: 48 - loss: 2.6202902793884277, accuracy: 0.5227272510528564\n",
      " Training epoch: 49 - loss: 2.5871574878692627, accuracy: 0.5227272510528564\n",
      " Training epoch: 50 - loss: 2.555068254470825, accuracy: 0.5227272510528564\n",
      " Training epoch: 51 - loss: 2.523983955383301, accuracy: 0.5227272510528564\n",
      " Training epoch: 52 - loss: 2.4938714504241943, accuracy: 0.5227272510528564\n",
      " Training epoch: 53 - loss: 2.464695453643799, accuracy: 0.5340909361839294\n",
      " Training epoch: 54 - loss: 2.436422348022461, accuracy: 0.5454545617103577\n",
      " Training epoch: 55 - loss: 2.40902042388916, accuracy: 0.5454545617103577\n",
      " Training epoch: 56 - loss: 2.382457733154297, accuracy: 0.5454545617103577\n",
      " Training epoch: 57 - loss: 2.3567047119140625, accuracy: 0.5568181872367859\n",
      " Training epoch: 58 - loss: 2.3317298889160156, accuracy: 0.5568181872367859\n",
      " Training epoch: 59 - loss: 2.307504415512085, accuracy: 0.5568181872367859\n",
      " Training epoch: 60 - loss: 2.2839982509613037, accuracy: 0.5681818127632141\n",
      " Training epoch: 61 - loss: 2.2611842155456543, accuracy: 0.5795454382896423\n",
      " Training epoch: 62 - loss: 2.239032506942749, accuracy: 0.5795454382896423\n",
      " Training epoch: 63 - loss: 2.2175161838531494, accuracy: 0.5795454382896423\n",
      " Training epoch: 64 - loss: 2.1966092586517334, accuracy: 0.5795454382896423\n",
      " Training epoch: 65 - loss: 2.1762871742248535, accuracy: 0.5795454382896423\n",
      " Training epoch: 66 - loss: 2.156524896621704, accuracy: 0.5795454382896423\n",
      " Training epoch: 67 - loss: 2.137298583984375, accuracy: 0.6022727489471436\n",
      " Training epoch: 68 - loss: 2.118587017059326, accuracy: 0.6022727489471436\n",
      " Training epoch: 69 - loss: 2.1003668308258057, accuracy: 0.6022727489471436\n",
      " Training epoch: 70 - loss: 2.0826189517974854, accuracy: 0.6022727489471436\n",
      " Training epoch: 71 - loss: 2.0653228759765625, accuracy: 0.6136363744735718\n",
      " Training epoch: 72 - loss: 2.0484628677368164, accuracy: 0.6136363744735718\n",
      " Training epoch: 73 - loss: 2.0320188999176025, accuracy: 0.6136363744735718\n",
      " Training epoch: 74 - loss: 2.0159761905670166, accuracy: 0.625\n",
      " Training epoch: 75 - loss: 2.000319004058838, accuracy: 0.6477272510528564\n",
      " Training epoch: 76 - loss: 1.9850318431854248, accuracy: 0.6477272510528564\n",
      " Training epoch: 77 - loss: 1.9701011180877686, accuracy: 0.6477272510528564\n",
      " Training epoch: 78 - loss: 1.9555147886276245, accuracy: 0.6477272510528564\n",
      " Training epoch: 79 - loss: 1.9412609338760376, accuracy: 0.6477272510528564\n",
      " Training epoch: 80 - loss: 1.9337310791015625, accuracy: 0.6477272510528564\n",
      " Training epoch: 81 - loss: 1.9267369508743286, accuracy: 0.6477272510528564\n",
      " Training epoch: 82 - loss: 1.9198323488235474, accuracy: 0.6477272510528564\n",
      " Training epoch: 83 - loss: 1.9130141735076904, accuracy: 0.6477272510528564\n",
      " Training epoch: 84 - loss: 1.9062790870666504, accuracy: 0.6477272510528564\n",
      " Training epoch: 85 - loss: 1.899623155593872, accuracy: 0.6477272510528564\n",
      " Training epoch: 86 - loss: 1.893045425415039, accuracy: 0.6477272510528564\n",
      " Training epoch: 87 - loss: 1.8865432739257812, accuracy: 0.6477272510528564\n",
      " Training epoch: 88 - loss: 1.8801137208938599, accuracy: 0.6477272510528564\n",
      " Training epoch: 89 - loss: 1.8737560510635376, accuracy: 0.6477272510528564\n",
      " Training epoch: 90 - loss: 1.8674685955047607, accuracy: 0.6477272510528564\n",
      " Training epoch: 91 - loss: 1.8612498044967651, accuracy: 0.6477272510528564\n",
      " Training epoch: 92 - loss: 1.8550972938537598, accuracy: 0.6477272510528564\n",
      " Training epoch: 93 - loss: 1.8490105867385864, accuracy: 0.6590909361839294\n",
      " Training epoch: 94 - loss: 1.8429890871047974, accuracy: 0.6590909361839294\n",
      " Training epoch: 95 - loss: 1.8370308876037598, accuracy: 0.6590909361839294\n",
      " Training epoch: 96 - loss: 1.8311352729797363, accuracy: 0.6590909361839294\n",
      " Training epoch: 97 - loss: 1.825300693511963, accuracy: 0.6590909361839294\n",
      " Training epoch: 98 - loss: 1.819525957107544, accuracy: 0.6590909361839294\n",
      " Training epoch: 99 - loss: 1.8138103485107422, accuracy: 0.6590909361839294\n",
      " Training epoch: 100 - loss: 1.8081539869308472, accuracy: 0.6590909361839294\n",
      " Training epoch: 101 - loss: 1.8025535345077515, accuracy: 0.6590909361839294\n",
      " Training epoch: 102 - loss: 1.7970099449157715, accuracy: 0.6590909361839294\n",
      " Training epoch: 103 - loss: 1.7915228605270386, accuracy: 0.6590909361839294\n",
      " Training epoch: 104 - loss: 1.7860897779464722, accuracy: 0.6590909361839294\n",
      " Training epoch: 105 - loss: 1.7807105779647827, accuracy: 0.6590909361839294\n",
      " Training epoch: 106 - loss: 1.775384545326233, accuracy: 0.6590909361839294\n",
      " Training epoch: 107 - loss: 1.7701114416122437, accuracy: 0.6590909361839294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training epoch: 108 - loss: 1.7648887634277344, accuracy: 0.6590909361839294\n",
      " Training epoch: 109 - loss: 1.7597180604934692, accuracy: 0.6590909361839294\n",
      " Training epoch: 110 - loss: 1.7545963525772095, accuracy: 0.6590909361839294\n",
      " Training epoch: 111 - loss: 1.7495250701904297, accuracy: 0.6590909361839294\n",
      " Training epoch: 112 - loss: 1.744502305984497, accuracy: 0.6590909361839294\n",
      " Training epoch: 113 - loss: 1.7395271062850952, accuracy: 0.6590909361839294\n",
      " Training epoch: 114 - loss: 1.7345997095108032, accuracy: 0.6704545617103577\n",
      " Training epoch: 115 - loss: 1.7297192811965942, accuracy: 0.6704545617103577\n",
      " Training epoch: 116 - loss: 1.724885106086731, accuracy: 0.6818181872367859\n",
      " Training epoch: 117 - loss: 1.720096230506897, accuracy: 0.6818181872367859\n",
      " Training epoch: 118 - loss: 1.715351939201355, accuracy: 0.6818181872367859\n",
      " Training epoch: 119 - loss: 1.710652470588684, accuracy: 0.6818181872367859\n",
      " Training epoch: 120 - loss: 1.705996036529541, accuracy: 0.6818181872367859\n",
      " Training epoch: 121 - loss: 1.701381802558899, accuracy: 0.6818181872367859\n",
      " Training epoch: 122 - loss: 1.6968109607696533, accuracy: 0.6818181872367859\n",
      " Training epoch: 123 - loss: 1.6922818422317505, accuracy: 0.6818181872367859\n",
      " Training epoch: 124 - loss: 1.6877940893173218, accuracy: 0.6818181872367859\n",
      " Training epoch: 125 - loss: 1.683347225189209, accuracy: 0.6818181872367859\n",
      " Training epoch: 126 - loss: 1.678941011428833, accuracy: 0.6818181872367859\n",
      " Training epoch: 127 - loss: 1.6745750904083252, accuracy: 0.6818181872367859\n",
      " Training epoch: 128 - loss: 1.6702481508255005, accuracy: 0.6818181872367859\n",
      " Training epoch: 129 - loss: 1.6659601926803589, accuracy: 0.6818181872367859\n",
      " Training epoch: 130 - loss: 1.6617103815078735, accuracy: 0.6818181872367859\n",
      " Training epoch: 131 - loss: 1.6574984788894653, accuracy: 0.6818181872367859\n",
      " Training epoch: 132 - loss: 1.6533244848251343, accuracy: 0.6818181872367859\n",
      " Training epoch: 133 - loss: 1.6491878032684326, accuracy: 0.6818181872367859\n",
      " Training epoch: 134 - loss: 1.6450867652893066, accuracy: 0.6818181872367859\n",
      " Training epoch: 135 - loss: 1.6410216093063354, accuracy: 0.6818181872367859\n",
      " Training epoch: 136 - loss: 1.6369926929473877, accuracy: 0.6818181872367859\n",
      " Training epoch: 137 - loss: 1.6329983472824097, accuracy: 0.6818181872367859\n",
      " Training epoch: 138 - loss: 1.6290379762649536, accuracy: 0.6818181872367859\n",
      " Training epoch: 139 - loss: 1.625112533569336, accuracy: 0.6818181872367859\n",
      " Training epoch: 140 - loss: 1.6212207078933716, accuracy: 0.6818181872367859\n",
      " Training epoch: 141 - loss: 1.6173629760742188, accuracy: 0.6818181872367859\n",
      " Training epoch: 142 - loss: 1.6135380268096924, accuracy: 0.6818181872367859\n",
      " Training epoch: 143 - loss: 1.6097453832626343, accuracy: 0.6818181872367859\n",
      " Training epoch: 144 - loss: 1.6059848070144653, accuracy: 0.6818181872367859\n",
      " Training epoch: 145 - loss: 1.6022553443908691, accuracy: 0.6818181872367859\n",
      " Training epoch: 146 - loss: 1.5985580682754517, accuracy: 0.6818181872367859\n",
      " Training epoch: 147 - loss: 1.5948920249938965, accuracy: 0.6818181872367859\n",
      " Training epoch: 148 - loss: 1.5912562608718872, accuracy: 0.6818181872367859\n",
      " Training epoch: 149 - loss: 1.587651014328003, accuracy: 0.6818181872367859\n",
      " Training epoch: 150 - loss: 1.5840752124786377, accuracy: 0.6818181872367859\n",
      " Training epoch: 151 - loss: 1.5805296897888184, accuracy: 0.6818181872367859\n",
      " Training epoch: 152 - loss: 1.5770134925842285, accuracy: 0.6818181872367859\n",
      " Training epoch: 153 - loss: 1.5735267400741577, accuracy: 0.6818181872367859\n",
      " Training epoch: 154 - loss: 1.5700684785842896, accuracy: 0.6818181872367859\n",
      " Training epoch: 155 - loss: 1.5666382312774658, accuracy: 0.6818181872367859\n",
      " Training epoch: 156 - loss: 1.5632362365722656, accuracy: 0.6818181872367859\n",
      " Training epoch: 157 - loss: 1.5598613023757935, accuracy: 0.6818181872367859\n",
      " Training epoch: 158 - loss: 1.556514024734497, accuracy: 0.6818181872367859\n",
      " Training epoch: 159 - loss: 1.5531935691833496, accuracy: 0.6818181872367859\n",
      " Training epoch: 160 - loss: 1.5499000549316406, accuracy: 0.6818181872367859\n",
      " Training epoch: 161 - loss: 1.5466324090957642, accuracy: 0.6818181872367859\n",
      " Training epoch: 162 - loss: 1.543391466140747, accuracy: 0.6818181872367859\n",
      " Training epoch: 163 - loss: 1.5401763916015625, accuracy: 0.6818181872367859\n",
      " Training epoch: 164 - loss: 1.5369868278503418, accuracy: 0.6818181872367859\n",
      " Training epoch: 165 - loss: 1.5338225364685059, accuracy: 0.6818181872367859\n",
      " Training epoch: 166 - loss: 1.5306823253631592, accuracy: 0.6818181872367859\n",
      " Training epoch: 167 - loss: 1.5275667905807495, accuracy: 0.6931818127632141\n",
      " Training epoch: 168 - loss: 1.5244758129119873, accuracy: 0.6931818127632141\n",
      " Training epoch: 169 - loss: 1.5214084386825562, accuracy: 0.6931818127632141\n",
      " Training epoch: 170 - loss: 1.5183653831481934, accuracy: 0.6931818127632141\n",
      " Training epoch: 171 - loss: 1.5153454542160034, accuracy: 0.6931818127632141\n",
      " Training epoch: 172 - loss: 1.512350082397461, accuracy: 0.6931818127632141\n",
      " Training epoch: 173 - loss: 1.5093772411346436, accuracy: 0.6931818127632141\n",
      " Training epoch: 174 - loss: 1.50642728805542, accuracy: 0.6931818127632141\n",
      " Training epoch: 175 - loss: 1.5035001039505005, accuracy: 0.6931818127632141\n",
      " Training epoch: 176 - loss: 1.5005950927734375, accuracy: 0.6931818127632141\n",
      " Training epoch: 177 - loss: 1.4977118968963623, accuracy: 0.6931818127632141\n",
      " Training epoch: 178 - loss: 1.4948498010635376, accuracy: 0.6931818127632141\n",
      " Training epoch: 179 - loss: 1.4920097589492798, accuracy: 0.6931818127632141\n",
      " Training epoch: 180 - loss: 1.4891905784606934, accuracy: 0.6931818127632141\n",
      " Training epoch: 181 - loss: 1.4863921403884888, accuracy: 0.6931818127632141\n",
      " Training epoch: 182 - loss: 1.4836146831512451, accuracy: 0.6931818127632141\n",
      " Training epoch: 183 - loss: 1.4808578491210938, accuracy: 0.6931818127632141\n",
      " Training epoch: 184 - loss: 1.4781216382980347, accuracy: 0.6931818127632141\n",
      " Training epoch: 185 - loss: 1.475405216217041, accuracy: 0.6931818127632141\n",
      " Training epoch: 186 - loss: 1.472708821296692, accuracy: 0.6931818127632141\n",
      " Training epoch: 187 - loss: 1.4700318574905396, accuracy: 0.6931818127632141\n",
      " Training epoch: 188 - loss: 1.4673746824264526, accuracy: 0.6931818127632141\n",
      " Training epoch: 189 - loss: 1.4647369384765625, accuracy: 0.6931818127632141\n",
      " Training epoch: 190 - loss: 1.462118148803711, accuracy: 0.6931818127632141\n",
      " Training epoch: 191 - loss: 1.4595187902450562, accuracy: 0.7045454382896423\n",
      " Training epoch: 192 - loss: 1.4569376707077026, accuracy: 0.7045454382896423\n",
      " Training epoch: 193 - loss: 1.4543747901916504, accuracy: 0.7045454382896423\n",
      " Training epoch: 194 - loss: 1.4518300294876099, accuracy: 0.7045454382896423\n",
      " Training epoch: 195 - loss: 1.4493030309677124, accuracy: 0.7045454382896423\n",
      " Training epoch: 196 - loss: 1.4467947483062744, accuracy: 0.7045454382896423\n",
      " Training epoch: 197 - loss: 1.4443037509918213, accuracy: 0.7045454382896423\n",
      " Training epoch: 198 - loss: 1.4418301582336426, accuracy: 0.7045454382896423\n",
      " Training epoch: 199 - loss: 1.4393742084503174, accuracy: 0.7045454382896423\n",
      "\n",
      "\n",
      " *****Test Accuracy: 0.7809000015258789*****\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 200\n",
    "with tf.Session() as sess:\n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch_i in range(epochs):\n",
    "        learning_rate = 1e-3 if epoch_i < 80 else 5e-4\n",
    "        # get batches\n",
    "        for batch_features, batch_labels in get_batches(batch_size, train_data_shape, train_labels):\n",
    "            _, score, loss = sess.run([optimizer, accuracy, cost], feed_dict={features:batch_features, \n",
    "                                                                              labels:batch_labels, \n",
    "                                                                              learning_rate_placeholder:learning_rate})\n",
    "        print(f\" Training epoch: {epoch_i} - loss: {loss}, accuracy: {score}\")\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={features:test_data_shape, labels:test_labels})\n",
    "    print(f\"\\n\\n *****Test Accuracy: {test_accuracy}*****\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As you can see accuracy tend to get stuck around epoch >170\n",
    "\n",
    "What we can do is normalize the inputs since we currently home something like below:\n",
    "![](figures/figure3.jpg)\n",
    "However, this is badly conditioned since it will be more difficult for our model to reduce the cost. Therefore we will work on a function to get something like below:\n",
    "![](figures/figure4.jpg)\n",
    "\n",
    "In our images this will mean that we should condition the pixels so that we get normalized values that are between certain boundaries [a,b]. To understand how to do this pleas go [here](https://en.wikipedia.org/wiki/Normalization_(statistics)#Examples)\n",
    "\n",
    "Implementing the Min-Max scaling function. The derivation of the formula is as follows:\n",
    "![](figures/figure5.jpg)\n",
    "\n",
    "Then the equation that we will implement is: \n",
    "\n",
    "$$\\boxed{X'=a+{\\frac {\\left(X-X_{\\min }\\right)\\left(b-a\\right)}{X_{\\max }-X_{\\min }}}}$$ with the parameters:\n",
    "\n",
    "$X_{\\min }=0$ //min of our current pixels\n",
    "\n",
    "$X_{\\max }=255$ //max of our current pixels\n",
    "\n",
    "$a=0.1$ //min scaled value desired\n",
    "\n",
    "$b=0.9$ //max scaled value desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_grayscale(image_data, a=0.1, b=0.9):\n",
    "    \"\"\"\n",
    "    Normalize the image data with Min-Max scaling to a range of [0.1, 0.9]\n",
    "    :param image_data: The image data to be normalized\n",
    "    :return: Normalized image data\n",
    "    \"\"\"\n",
    "    grayscale_min = np.min(image_data)\n",
    "    grayscale_max = np.max(image_data)\n",
    "    return a + (((image_data - grayscale_min)*(b - a) )/(grayscale_max - grayscale_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train_data = normalize_grayscale(train_data_shape)\n",
    "norm_test_data = normalize_grayscale(test_data_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training epoch: 0 - loss: 8.090409278869629, accuracy: 0.07954545319080353\n",
      " Training epoch: 1 - loss: 7.378990650177002, accuracy: 0.09090909361839294\n",
      " Training epoch: 2 - loss: 6.834489345550537, accuracy: 0.125\n",
      " Training epoch: 3 - loss: 6.412035942077637, accuracy: 0.17045454680919647\n",
      " Training epoch: 4 - loss: 6.076138496398926, accuracy: 0.17045454680919647\n",
      " Training epoch: 5 - loss: 5.8068037033081055, accuracy: 0.20454545319080353\n",
      " Training epoch: 6 - loss: 5.58450984954834, accuracy: 0.21590909361839294\n",
      " Training epoch: 7 - loss: 5.390984058380127, accuracy: 0.22727273404598236\n",
      " Training epoch: 8 - loss: 5.215121269226074, accuracy: 0.22727273404598236\n",
      " Training epoch: 9 - loss: 5.051466464996338, accuracy: 0.22727273404598236\n",
      " Training epoch: 10 - loss: 4.897456169128418, accuracy: 0.22727273404598236\n",
      " Training epoch: 11 - loss: 4.751826763153076, accuracy: 0.21590909361839294\n",
      " Training epoch: 12 - loss: 4.613865375518799, accuracy: 0.22727273404598236\n",
      " Training epoch: 13 - loss: 4.483099460601807, accuracy: 0.23863635957241058\n",
      " Training epoch: 14 - loss: 4.359148025512695, accuracy: 0.2613636255264282\n",
      " Training epoch: 15 - loss: 4.2416582107543945, accuracy: 0.2613636255264282\n",
      " Training epoch: 16 - loss: 4.130282878875732, accuracy: 0.2613636255264282\n",
      " Training epoch: 17 - loss: 4.024676322937012, accuracy: 0.27272728085517883\n",
      " Training epoch: 18 - loss: 3.9245071411132812, accuracy: 0.3068181872367859\n",
      " Training epoch: 19 - loss: 3.829435110092163, accuracy: 0.3181818127632141\n",
      " Training epoch: 20 - loss: 3.7391357421875, accuracy: 0.3181818127632141\n",
      " Training epoch: 21 - loss: 3.6532914638519287, accuracy: 0.3181818127632141\n",
      " Training epoch: 22 - loss: 3.5716047286987305, accuracy: 0.3068181872367859\n",
      " Training epoch: 23 - loss: 3.49379825592041, accuracy: 0.3068181872367859\n",
      " Training epoch: 24 - loss: 3.419623613357544, accuracy: 0.3068181872367859\n",
      " Training epoch: 25 - loss: 3.3488502502441406, accuracy: 0.3181818127632141\n",
      " Training epoch: 26 - loss: 3.281266927719116, accuracy: 0.3181818127632141\n",
      " Training epoch: 27 - loss: 3.216684579849243, accuracy: 0.3181818127632141\n",
      " Training epoch: 28 - loss: 3.1549301147460938, accuracy: 0.3181818127632141\n",
      " Training epoch: 29 - loss: 3.095841884613037, accuracy: 0.3295454680919647\n",
      " Training epoch: 30 - loss: 3.039273262023926, accuracy: 0.3295454680919647\n",
      " Training epoch: 31 - loss: 2.9850897789001465, accuracy: 0.3295454680919647\n",
      " Training epoch: 32 - loss: 2.933164358139038, accuracy: 0.34090909361839294\n",
      " Training epoch: 33 - loss: 2.8833744525909424, accuracy: 0.34090909361839294\n",
      " Training epoch: 34 - loss: 2.8356094360351562, accuracy: 0.34090909361839294\n",
      " Training epoch: 35 - loss: 2.7897636890411377, accuracy: 0.3636363744735718\n",
      " Training epoch: 36 - loss: 2.745731830596924, accuracy: 0.3636363744735718\n",
      " Training epoch: 37 - loss: 2.7034244537353516, accuracy: 0.375\n",
      " Training epoch: 38 - loss: 2.6627442836761475, accuracy: 0.39772728085517883\n",
      " Training epoch: 39 - loss: 2.623610496520996, accuracy: 0.40909090638160706\n",
      " Training epoch: 40 - loss: 2.5859391689300537, accuracy: 0.4204545319080353\n",
      " Training epoch: 41 - loss: 2.5496559143066406, accuracy: 0.4431818127632141\n",
      " Training epoch: 42 - loss: 2.514686346054077, accuracy: 0.4545454680919647\n",
      " Training epoch: 43 - loss: 2.4809629917144775, accuracy: 0.4545454680919647\n",
      " Training epoch: 44 - loss: 2.4484221935272217, accuracy: 0.4545454680919647\n",
      " Training epoch: 45 - loss: 2.417004346847534, accuracy: 0.46590909361839294\n",
      " Training epoch: 46 - loss: 2.386650562286377, accuracy: 0.5113636255264282\n",
      " Training epoch: 47 - loss: 2.357309103012085, accuracy: 0.5113636255264282\n",
      " Training epoch: 48 - loss: 2.328930616378784, accuracy: 0.5113636255264282\n",
      " Training epoch: 49 - loss: 2.3014676570892334, accuracy: 0.5227272510528564\n",
      " Training epoch: 50 - loss: 2.274876356124878, accuracy: 0.5227272510528564\n",
      " Training epoch: 51 - loss: 2.2491164207458496, accuracy: 0.5227272510528564\n",
      " Training epoch: 52 - loss: 2.224147081375122, accuracy: 0.5340909361839294\n",
      " Training epoch: 53 - loss: 2.199932813644409, accuracy: 0.5340909361839294\n",
      " Training epoch: 54 - loss: 2.1764369010925293, accuracy: 0.5454545617103577\n",
      " Training epoch: 55 - loss: 2.153630256652832, accuracy: 0.5454545617103577\n",
      " Training epoch: 56 - loss: 2.1314780712127686, accuracy: 0.5454545617103577\n",
      " Training epoch: 57 - loss: 2.1099536418914795, accuracy: 0.5681818127632141\n",
      " Training epoch: 58 - loss: 2.0890281200408936, accuracy: 0.5681818127632141\n",
      " Training epoch: 59 - loss: 2.0686769485473633, accuracy: 0.5681818127632141\n",
      " Training epoch: 60 - loss: 2.0488717555999756, accuracy: 0.5681818127632141\n",
      " Training epoch: 61 - loss: 2.0295915603637695, accuracy: 0.5909090638160706\n",
      " Training epoch: 62 - loss: 2.0108144283294678, accuracy: 0.5909090638160706\n",
      " Training epoch: 63 - loss: 1.9925167560577393, accuracy: 0.5909090638160706\n",
      " Training epoch: 64 - loss: 1.9746800661087036, accuracy: 0.5909090638160706\n",
      " Training epoch: 65 - loss: 1.957284927368164, accuracy: 0.5909090638160706\n",
      " Training epoch: 66 - loss: 1.9403133392333984, accuracy: 0.5909090638160706\n",
      " Training epoch: 67 - loss: 1.923749566078186, accuracy: 0.5909090638160706\n",
      " Training epoch: 68 - loss: 1.9075767993927002, accuracy: 0.6022727489471436\n",
      " Training epoch: 69 - loss: 1.891778588294983, accuracy: 0.6022727489471436\n",
      " Training epoch: 70 - loss: 1.8763407468795776, accuracy: 0.6022727489471436\n",
      " Training epoch: 71 - loss: 1.8612478971481323, accuracy: 0.6022727489471436\n",
      " Training epoch: 72 - loss: 1.84648859500885, accuracy: 0.6022727489471436\n",
      " Training epoch: 73 - loss: 1.8320492506027222, accuracy: 0.6022727489471436\n",
      " Training epoch: 74 - loss: 1.817918300628662, accuracy: 0.6022727489471436\n",
      " Training epoch: 75 - loss: 1.804082989692688, accuracy: 0.6022727489471436\n",
      " Training epoch: 76 - loss: 1.7905349731445312, accuracy: 0.6022727489471436\n",
      " Training epoch: 77 - loss: 1.7772637605667114, accuracy: 0.6022727489471436\n",
      " Training epoch: 78 - loss: 1.7642573118209839, accuracy: 0.6022727489471436\n",
      " Training epoch: 79 - loss: 1.7515106201171875, accuracy: 0.6022727489471436\n",
      " Training epoch: 80 - loss: 1.7390094995498657, accuracy: 0.6022727489471436\n",
      " Training epoch: 81 - loss: 1.7267471551895142, accuracy: 0.6022727489471436\n",
      " Training epoch: 82 - loss: 1.714715838432312, accuracy: 0.6022727489471436\n",
      " Training epoch: 83 - loss: 1.7029082775115967, accuracy: 0.6022727489471436\n",
      " Training epoch: 84 - loss: 1.6913164854049683, accuracy: 0.6022727489471436\n",
      " Training epoch: 85 - loss: 1.6799343824386597, accuracy: 0.6022727489471436\n",
      " Training epoch: 86 - loss: 1.6687538623809814, accuracy: 0.6022727489471436\n",
      " Training epoch: 87 - loss: 1.6577705144882202, accuracy: 0.6022727489471436\n",
      " Training epoch: 88 - loss: 1.6469768285751343, accuracy: 0.6022727489471436\n",
      " Training epoch: 89 - loss: 1.6363677978515625, accuracy: 0.6022727489471436\n",
      " Training epoch: 90 - loss: 1.6259368658065796, accuracy: 0.6022727489471436\n",
      " Training epoch: 91 - loss: 1.6156789064407349, accuracy: 0.6022727489471436\n",
      " Training epoch: 92 - loss: 1.6055901050567627, accuracy: 0.6022727489471436\n",
      " Training epoch: 93 - loss: 1.5956635475158691, accuracy: 0.6022727489471436\n",
      " Training epoch: 94 - loss: 1.5858958959579468, accuracy: 0.6022727489471436\n",
      " Training epoch: 95 - loss: 1.5762814283370972, accuracy: 0.6022727489471436\n",
      " Training epoch: 96 - loss: 1.5668185949325562, accuracy: 0.6022727489471436\n",
      " Training epoch: 97 - loss: 1.557501196861267, accuracy: 0.6022727489471436\n",
      " Training epoch: 98 - loss: 1.5483242273330688, accuracy: 0.6022727489471436\n",
      " Training epoch: 99 - loss: 1.5392855405807495, accuracy: 0.6136363744735718\n",
      " Training epoch: 100 - loss: 1.5303806066513062, accuracy: 0.6136363744735718\n",
      " Training epoch: 101 - loss: 1.5216069221496582, accuracy: 0.625\n",
      " Training epoch: 102 - loss: 1.5129603147506714, accuracy: 0.625\n",
      " Training epoch: 103 - loss: 1.50443696975708, accuracy: 0.625\n",
      " Training epoch: 104 - loss: 1.4960349798202515, accuracy: 0.625\n",
      " Training epoch: 105 - loss: 1.4877513647079468, accuracy: 0.625\n",
      " Training epoch: 106 - loss: 1.4795829057693481, accuracy: 0.625\n",
      " Training epoch: 107 - loss: 1.4715259075164795, accuracy: 0.625\n",
      " Training epoch: 108 - loss: 1.463578701019287, accuracy: 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training epoch: 109 - loss: 1.4557398557662964, accuracy: 0.625\n",
      " Training epoch: 110 - loss: 1.4480037689208984, accuracy: 0.625\n",
      " Training epoch: 111 - loss: 1.4403706789016724, accuracy: 0.6363636255264282\n",
      " Training epoch: 112 - loss: 1.4328364133834839, accuracy: 0.6363636255264282\n",
      " Training epoch: 113 - loss: 1.425400972366333, accuracy: 0.6363636255264282\n",
      " Training epoch: 114 - loss: 1.418059229850769, accuracy: 0.6363636255264282\n",
      " Training epoch: 115 - loss: 1.4108116626739502, accuracy: 0.6363636255264282\n",
      " Training epoch: 116 - loss: 1.4036542177200317, accuracy: 0.6363636255264282\n",
      " Training epoch: 117 - loss: 1.396586537361145, accuracy: 0.6590909361839294\n",
      " Training epoch: 118 - loss: 1.3896052837371826, accuracy: 0.6704545617103577\n",
      " Training epoch: 119 - loss: 1.3827104568481445, accuracy: 0.6704545617103577\n",
      " Training epoch: 120 - loss: 1.3758999109268188, accuracy: 0.6704545617103577\n",
      " Training epoch: 121 - loss: 1.3691712617874146, accuracy: 0.6704545617103577\n",
      " Training epoch: 122 - loss: 1.3625218868255615, accuracy: 0.6704545617103577\n",
      " Training epoch: 123 - loss: 1.3559517860412598, accuracy: 0.6818181872367859\n",
      " Training epoch: 124 - loss: 1.3494582176208496, accuracy: 0.6818181872367859\n",
      " Training epoch: 125 - loss: 1.343040108680725, accuracy: 0.6818181872367859\n",
      " Training epoch: 126 - loss: 1.3366961479187012, accuracy: 0.6818181872367859\n",
      " Training epoch: 127 - loss: 1.3304234743118286, accuracy: 0.6818181872367859\n",
      " Training epoch: 128 - loss: 1.324223279953003, accuracy: 0.6818181872367859\n",
      " Training epoch: 129 - loss: 1.3180922269821167, accuracy: 0.6818181872367859\n",
      " Training epoch: 130 - loss: 1.31203031539917, accuracy: 0.6818181872367859\n",
      " Training epoch: 131 - loss: 1.3060358762741089, accuracy: 0.6818181872367859\n",
      " Training epoch: 132 - loss: 1.3001075983047485, accuracy: 0.6818181872367859\n",
      " Training epoch: 133 - loss: 1.2942439317703247, accuracy: 0.6818181872367859\n",
      " Training epoch: 134 - loss: 1.2884440422058105, accuracy: 0.6818181872367859\n",
      " Training epoch: 135 - loss: 1.2827062606811523, accuracy: 0.6818181872367859\n",
      " Training epoch: 136 - loss: 1.2770285606384277, accuracy: 0.6818181872367859\n",
      " Training epoch: 137 - loss: 1.2714133262634277, accuracy: 0.6818181872367859\n",
      " Training epoch: 138 - loss: 1.2658576965332031, accuracy: 0.6818181872367859\n",
      " Training epoch: 139 - loss: 1.2603594064712524, accuracy: 0.6818181872367859\n",
      " Training epoch: 140 - loss: 1.2549190521240234, accuracy: 0.6818181872367859\n",
      " Training epoch: 141 - loss: 1.2495347261428833, accuracy: 0.6818181872367859\n",
      " Training epoch: 142 - loss: 1.2442060708999634, accuracy: 0.6818181872367859\n",
      " Training epoch: 143 - loss: 1.2389317750930786, accuracy: 0.6818181872367859\n",
      " Training epoch: 144 - loss: 1.23371160030365, accuracy: 0.6818181872367859\n",
      " Training epoch: 145 - loss: 1.2285445928573608, accuracy: 0.6931818127632141\n",
      " Training epoch: 146 - loss: 1.2234289646148682, accuracy: 0.6931818127632141\n",
      " Training epoch: 147 - loss: 1.2183629274368286, accuracy: 0.6931818127632141\n",
      " Training epoch: 148 - loss: 1.213348388671875, accuracy: 0.6931818127632141\n",
      " Training epoch: 149 - loss: 1.2083836793899536, accuracy: 0.6931818127632141\n",
      " Training epoch: 150 - loss: 1.2034673690795898, accuracy: 0.6931818127632141\n",
      " Training epoch: 151 - loss: 1.1985989809036255, accuracy: 0.6931818127632141\n",
      " Training epoch: 152 - loss: 1.1937782764434814, accuracy: 0.6931818127632141\n",
      " Training epoch: 153 - loss: 1.189005732536316, accuracy: 0.6931818127632141\n",
      " Training epoch: 154 - loss: 1.184277057647705, accuracy: 0.6931818127632141\n",
      " Training epoch: 155 - loss: 1.1795945167541504, accuracy: 0.6931818127632141\n",
      " Training epoch: 156 - loss: 1.1749576330184937, accuracy: 0.6931818127632141\n",
      " Training epoch: 157 - loss: 1.1703636646270752, accuracy: 0.6931818127632141\n",
      " Training epoch: 158 - loss: 1.16581392288208, accuracy: 0.6931818127632141\n",
      " Training epoch: 159 - loss: 1.1613060235977173, accuracy: 0.6931818127632141\n",
      " Training epoch: 160 - loss: 1.1568406820297241, accuracy: 0.6931818127632141\n",
      " Training epoch: 161 - loss: 1.1524173021316528, accuracy: 0.6931818127632141\n",
      " Training epoch: 162 - loss: 1.148034691810608, accuracy: 0.6931818127632141\n",
      " Training epoch: 163 - loss: 1.1436935663223267, accuracy: 0.6931818127632141\n",
      " Training epoch: 164 - loss: 1.1393905878067017, accuracy: 0.6931818127632141\n",
      " Training epoch: 165 - loss: 1.1351280212402344, accuracy: 0.6931818127632141\n",
      " Training epoch: 166 - loss: 1.1309033632278442, accuracy: 0.6931818127632141\n",
      " Training epoch: 167 - loss: 1.126718282699585, accuracy: 0.6931818127632141\n",
      " Training epoch: 168 - loss: 1.1225703954696655, accuracy: 0.6931818127632141\n",
      " Training epoch: 169 - loss: 1.1184595823287964, accuracy: 0.6931818127632141\n",
      " Training epoch: 170 - loss: 1.1143858432769775, accuracy: 0.6931818127632141\n",
      " Training epoch: 171 - loss: 1.1103484630584717, accuracy: 0.6931818127632141\n",
      " Training epoch: 172 - loss: 1.1063472032546997, accuracy: 0.6931818127632141\n",
      " Training epoch: 173 - loss: 1.1023837327957153, accuracy: 0.6931818127632141\n",
      " Training epoch: 174 - loss: 1.098454236984253, accuracy: 0.6931818127632141\n",
      " Training epoch: 175 - loss: 1.0945594310760498, accuracy: 0.6931818127632141\n",
      " Training epoch: 176 - loss: 1.0906981229782104, accuracy: 0.6931818127632141\n",
      " Training epoch: 177 - loss: 1.0868712663650513, accuracy: 0.6931818127632141\n",
      " Training epoch: 178 - loss: 1.0830782651901245, accuracy: 0.6931818127632141\n",
      " Training epoch: 179 - loss: 1.079317569732666, accuracy: 0.6931818127632141\n",
      " Training epoch: 180 - loss: 1.0755895376205444, accuracy: 0.6931818127632141\n",
      " Training epoch: 181 - loss: 1.0718938112258911, accuracy: 0.6931818127632141\n",
      " Training epoch: 182 - loss: 1.0682307481765747, accuracy: 0.6931818127632141\n",
      " Training epoch: 183 - loss: 1.0645979642868042, accuracy: 0.6931818127632141\n",
      " Training epoch: 184 - loss: 1.0609959363937378, accuracy: 0.6931818127632141\n",
      " Training epoch: 185 - loss: 1.05742609500885, accuracy: 0.6931818127632141\n",
      " Training epoch: 186 - loss: 1.0538862943649292, accuracy: 0.6931818127632141\n",
      " Training epoch: 187 - loss: 1.0503748655319214, accuracy: 0.6931818127632141\n",
      " Training epoch: 188 - loss: 1.046895146369934, accuracy: 0.6931818127632141\n",
      " Training epoch: 189 - loss: 1.0434448719024658, accuracy: 0.6931818127632141\n",
      " Training epoch: 190 - loss: 1.0400227308273315, accuracy: 0.6931818127632141\n",
      " Training epoch: 191 - loss: 1.0366291999816895, accuracy: 0.6931818127632141\n",
      " Training epoch: 192 - loss: 1.0332636833190918, accuracy: 0.6931818127632141\n",
      " Training epoch: 193 - loss: 1.029924988746643, accuracy: 0.6931818127632141\n",
      " Training epoch: 194 - loss: 1.0266166925430298, accuracy: 0.6931818127632141\n",
      " Training epoch: 195 - loss: 1.023335576057434, accuracy: 0.6931818127632141\n",
      " Training epoch: 196 - loss: 1.0200812816619873, accuracy: 0.6931818127632141\n",
      " Training epoch: 197 - loss: 1.0168533325195312, accuracy: 0.6931818127632141\n",
      " Training epoch: 198 - loss: 1.0136514902114868, accuracy: 0.6931818127632141\n",
      " Training epoch: 199 - loss: 1.0104775428771973, accuracy: 0.6931818127632141\n",
      "\n",
      "\n",
      " *****Test Accuracy: 0.7953000068664551*****\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 200\n",
    "with tf.Session() as sess:\n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch_i in range(epochs):\n",
    "        learning_rate = 1e-3 #if epoch_i < 80 else 5e-4\n",
    "        # get batches\n",
    "        for batch_features, batch_labels in get_batches(batch_size, norm_train_data, train_labels):\n",
    "            _, score, loss = sess.run([optimizer, accuracy, cost], feed_dict={features:batch_features, \n",
    "                                                                              labels:batch_labels, \n",
    "                                                                              learning_rate_placeholder:learning_rate})\n",
    "        print(f\" Training epoch: {epoch_i} - loss: {loss}, accuracy: {score}\")\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={features:norm_test_data, labels:test_labels})\n",
    "    print(f\"\\n\\n *****Test Accuracy: {test_accuracy}*****\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** we can see that the accuracy increases a little bit faster than without normalization\n",
    "\n",
    "## Trying ADAGRAD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training epoch: 0 - loss: 3.6356382369995117, accuracy: 0.3295454680919647\n",
      " Training epoch: 1 - loss: 2.6026272773742676, accuracy: 0.4545454680919647\n",
      " Training epoch: 2 - loss: 2.1075005531311035, accuracy: 0.5\n",
      " Training epoch: 3 - loss: 1.8105731010437012, accuracy: 0.5340909361839294\n",
      " Training epoch: 4 - loss: 1.6073112487792969, accuracy: 0.5568181872367859\n",
      " Training epoch: 5 - loss: 1.4579771757125854, accuracy: 0.6136363744735718\n",
      " Training epoch: 6 - loss: 1.3432120084762573, accuracy: 0.6818181872367859\n",
      " Training epoch: 7 - loss: 1.252350926399231, accuracy: 0.6818181872367859\n",
      " Training epoch: 8 - loss: 1.1787794828414917, accuracy: 0.6931818127632141\n",
      " Training epoch: 9 - loss: 1.1181049346923828, accuracy: 0.6931818127632141\n",
      " Training epoch: 10 - loss: 1.0672802925109863, accuracy: 0.7045454382896423\n",
      " Training epoch: 11 - loss: 1.0241230726242065, accuracy: 0.7045454382896423\n",
      " Training epoch: 12 - loss: 0.9870288372039795, accuracy: 0.7272727489471436\n",
      " Training epoch: 13 - loss: 0.9548008441925049, accuracy: 0.7386363744735718\n",
      " Training epoch: 14 - loss: 0.9265302419662476, accuracy: 0.7386363744735718\n",
      " Training epoch: 15 - loss: 0.9015205502510071, accuracy: 0.7727272510528564\n",
      " Training epoch: 16 - loss: 0.8792263269424438, accuracy: 0.7727272510528564\n",
      " Training epoch: 17 - loss: 0.8592162728309631, accuracy: 0.7840909361839294\n",
      " Training epoch: 18 - loss: 0.8411461114883423, accuracy: 0.7954545617103577\n",
      " Training epoch: 19 - loss: 0.8247345089912415, accuracy: 0.8068181872367859\n",
      " Training epoch: 20 - loss: 0.8097537755966187, accuracy: 0.8068181872367859\n",
      " Training epoch: 21 - loss: 0.7960141897201538, accuracy: 0.8068181872367859\n",
      " Training epoch: 22 - loss: 0.7833572030067444, accuracy: 0.8181818127632141\n",
      " Training epoch: 23 - loss: 0.7716525793075562, accuracy: 0.8181818127632141\n",
      " Training epoch: 24 - loss: 0.7607879638671875, accuracy: 0.8295454382896423\n",
      " Training epoch: 25 - loss: 0.7506692409515381, accuracy: 0.8295454382896423\n",
      " Training epoch: 26 - loss: 0.7412153482437134, accuracy: 0.8295454382896423\n",
      " Training epoch: 27 - loss: 0.7323558926582336, accuracy: 0.8409090638160706\n",
      " Training epoch: 28 - loss: 0.7240320444107056, accuracy: 0.8409090638160706\n",
      " Training epoch: 29 - loss: 0.7161908149719238, accuracy: 0.8409090638160706\n",
      " Training epoch: 30 - loss: 0.7087863087654114, accuracy: 0.8409090638160706\n",
      " Training epoch: 31 - loss: 0.7017790079116821, accuracy: 0.8409090638160706\n",
      " Training epoch: 32 - loss: 0.695133626461029, accuracy: 0.8409090638160706\n",
      " Training epoch: 33 - loss: 0.6888194680213928, accuracy: 0.8409090638160706\n",
      " Training epoch: 34 - loss: 0.6828083395957947, accuracy: 0.8409090638160706\n",
      " Training epoch: 35 - loss: 0.6770763397216797, accuracy: 0.8409090638160706\n",
      " Training epoch: 36 - loss: 0.6716013550758362, accuracy: 0.8522727489471436\n",
      " Training epoch: 37 - loss: 0.6663638949394226, accuracy: 0.8522727489471436\n",
      " Training epoch: 38 - loss: 0.6613456606864929, accuracy: 0.8522727489471436\n",
      " Training epoch: 39 - loss: 0.6565313935279846, accuracy: 0.8522727489471436\n",
      " Training epoch: 40 - loss: 0.6519066691398621, accuracy: 0.8522727489471436\n",
      " Training epoch: 41 - loss: 0.6474586129188538, accuracy: 0.8522727489471436\n",
      " Training epoch: 42 - loss: 0.6431753039360046, accuracy: 0.8522727489471436\n",
      " Training epoch: 43 - loss: 0.6390457153320312, accuracy: 0.8522727489471436\n",
      " Training epoch: 44 - loss: 0.6350610256195068, accuracy: 0.8522727489471436\n",
      " Training epoch: 45 - loss: 0.6312116980552673, accuracy: 0.8522727489471436\n",
      " Training epoch: 46 - loss: 0.6274898052215576, accuracy: 0.8522727489471436\n",
      " Training epoch: 47 - loss: 0.6238878965377808, accuracy: 0.8522727489471436\n",
      " Training epoch: 48 - loss: 0.6203978061676025, accuracy: 0.8522727489471436\n",
      " Training epoch: 49 - loss: 0.6170154213905334, accuracy: 0.8522727489471436\n",
      " Training epoch: 50 - loss: 0.6137335896492004, accuracy: 0.8522727489471436\n",
      " Training epoch: 51 - loss: 0.6105470657348633, accuracy: 0.8522727489471436\n",
      " Training epoch: 52 - loss: 0.6074508428573608, accuracy: 0.8522727489471436\n",
      " Training epoch: 53 - loss: 0.604441225528717, accuracy: 0.8522727489471436\n",
      " Training epoch: 54 - loss: 0.6015121340751648, accuracy: 0.8522727489471436\n",
      " Training epoch: 55 - loss: 0.5986614227294922, accuracy: 0.8522727489471436\n",
      " Training epoch: 56 - loss: 0.5958839058876038, accuracy: 0.8522727489471436\n",
      " Training epoch: 57 - loss: 0.5931769013404846, accuracy: 0.8522727489471436\n",
      " Training epoch: 58 - loss: 0.5905369520187378, accuracy: 0.8522727489471436\n",
      " Training epoch: 59 - loss: 0.5879616141319275, accuracy: 0.8522727489471436\n",
      " Training epoch: 60 - loss: 0.5854474902153015, accuracy: 0.8522727489471436\n",
      " Training epoch: 61 - loss: 0.5829915404319763, accuracy: 0.8522727489471436\n",
      " Training epoch: 62 - loss: 0.5805925726890564, accuracy: 0.8522727489471436\n",
      " Training epoch: 63 - loss: 0.5782468914985657, accuracy: 0.8522727489471436\n",
      " Training epoch: 64 - loss: 0.5759531259536743, accuracy: 0.8522727489471436\n",
      " Training epoch: 65 - loss: 0.5737087726593018, accuracy: 0.8522727489471436\n",
      " Training epoch: 66 - loss: 0.5715123414993286, accuracy: 0.8522727489471436\n",
      " Training epoch: 67 - loss: 0.569362223148346, accuracy: 0.8522727489471436\n",
      " Training epoch: 68 - loss: 0.5672553777694702, accuracy: 0.8522727489471436\n",
      " Training epoch: 69 - loss: 0.5651914477348328, accuracy: 0.8522727489471436\n",
      " Training epoch: 70 - loss: 0.5631683468818665, accuracy: 0.8522727489471436\n",
      " Training epoch: 71 - loss: 0.5611852407455444, accuracy: 0.8522727489471436\n",
      " Training epoch: 72 - loss: 0.5592396259307861, accuracy: 0.8522727489471436\n",
      " Training epoch: 73 - loss: 0.557330846786499, accuracy: 0.8522727489471436\n",
      " Training epoch: 74 - loss: 0.5554578900337219, accuracy: 0.8522727489471436\n",
      " Training epoch: 75 - loss: 0.5536186099052429, accuracy: 0.8522727489471436\n",
      " Training epoch: 76 - loss: 0.5518127679824829, accuracy: 0.8522727489471436\n",
      " Training epoch: 77 - loss: 0.5500393509864807, accuracy: 0.8522727489471436\n",
      " Training epoch: 78 - loss: 0.5482968688011169, accuracy: 0.8522727489471436\n",
      " Training epoch: 79 - loss: 0.5465848445892334, accuracy: 0.8522727489471436\n",
      " Training epoch: 80 - loss: 0.5449017882347107, accuracy: 0.8522727489471436\n",
      " Training epoch: 81 - loss: 0.5432469844818115, accuracy: 0.8522727489471436\n",
      " Training epoch: 82 - loss: 0.5416191220283508, accuracy: 0.8522727489471436\n",
      " Training epoch: 83 - loss: 0.5400183200836182, accuracy: 0.8522727489471436\n",
      " Training epoch: 84 - loss: 0.5384437441825867, accuracy: 0.8522727489471436\n",
      " Training epoch: 85 - loss: 0.5368938446044922, accuracy: 0.8522727489471436\n",
      " Training epoch: 86 - loss: 0.5353685617446899, accuracy: 0.8522727489471436\n",
      " Training epoch: 87 - loss: 0.5338671207427979, accuracy: 0.8522727489471436\n",
      " Training epoch: 88 - loss: 0.5323886871337891, accuracy: 0.8522727489471436\n",
      " Training epoch: 89 - loss: 0.5309324860572815, accuracy: 0.8522727489471436\n",
      " Training epoch: 90 - loss: 0.5294979214668274, accuracy: 0.8522727489471436\n",
      " Training epoch: 91 - loss: 0.5280846953392029, accuracy: 0.8522727489471436\n",
      " Training epoch: 92 - loss: 0.5266921520233154, accuracy: 0.8522727489471436\n",
      " Training epoch: 93 - loss: 0.5253198742866516, accuracy: 0.8522727489471436\n",
      " Training epoch: 94 - loss: 0.5239671468734741, accuracy: 0.8522727489471436\n",
      " Training epoch: 95 - loss: 0.5226331949234009, accuracy: 0.8522727489471436\n",
      " Training epoch: 96 - loss: 0.5213180184364319, accuracy: 0.8522727489471436\n",
      " Training epoch: 97 - loss: 0.5200209021568298, accuracy: 0.8522727489471436\n",
      " Training epoch: 98 - loss: 0.5187416672706604, accuracy: 0.8522727489471436\n",
      " Training epoch: 99 - loss: 0.5174798369407654, accuracy: 0.8522727489471436\n",
      " Training epoch: 100 - loss: 0.5162351131439209, accuracy: 0.8522727489471436\n",
      " Training epoch: 101 - loss: 0.5150063037872314, accuracy: 0.8522727489471436\n",
      " Training epoch: 102 - loss: 0.5137937664985657, accuracy: 0.8522727489471436\n",
      " Training epoch: 103 - loss: 0.5125970244407654, accuracy: 0.8522727489471436\n",
      " Training epoch: 104 - loss: 0.5114156603813171, accuracy: 0.8522727489471436\n",
      " Training epoch: 105 - loss: 0.5102493166923523, accuracy: 0.8522727489471436\n",
      " Training epoch: 106 - loss: 0.5090981125831604, accuracy: 0.8522727489471436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training epoch: 107 - loss: 0.5079606771469116, accuracy: 0.8522727489471436\n",
      " Training epoch: 108 - loss: 0.5068378448486328, accuracy: 0.8522727489471436\n",
      " Training epoch: 109 - loss: 0.5057283043861389, accuracy: 0.8522727489471436\n",
      " Training epoch: 110 - loss: 0.5046325922012329, accuracy: 0.8522727489471436\n",
      " Training epoch: 111 - loss: 0.5035499334335327, accuracy: 0.8522727489471436\n",
      " Training epoch: 112 - loss: 0.5024800896644592, accuracy: 0.8522727489471436\n",
      " Training epoch: 113 - loss: 0.5014229416847229, accuracy: 0.8522727489471436\n",
      " Training epoch: 114 - loss: 0.5003781914710999, accuracy: 0.8522727489471436\n",
      " Training epoch: 115 - loss: 0.49934494495391846, accuracy: 0.8522727489471436\n",
      " Training epoch: 116 - loss: 0.49832451343536377, accuracy: 0.8522727489471436\n",
      " Training epoch: 117 - loss: 0.4973154067993164, accuracy: 0.8522727489471436\n",
      " Training epoch: 118 - loss: 0.4963182210922241, accuracy: 0.8522727489471436\n",
      " Training epoch: 119 - loss: 0.49533167481422424, accuracy: 0.8522727489471436\n",
      " Training epoch: 120 - loss: 0.4943561553955078, accuracy: 0.8636363744735718\n",
      " Training epoch: 121 - loss: 0.4933914244174957, accuracy: 0.8636363744735718\n",
      " Training epoch: 122 - loss: 0.49243709444999695, accuracy: 0.8636363744735718\n",
      " Training epoch: 123 - loss: 0.4914934039115906, accuracy: 0.8636363744735718\n",
      " Training epoch: 124 - loss: 0.49055948853492737, accuracy: 0.8636363744735718\n",
      " Training epoch: 125 - loss: 0.48963606357574463, accuracy: 0.8636363744735718\n",
      " Training epoch: 126 - loss: 0.48872262239456177, accuracy: 0.8636363744735718\n",
      " Training epoch: 127 - loss: 0.4878183603286743, accuracy: 0.8636363744735718\n",
      " Training epoch: 128 - loss: 0.4869237244129181, accuracy: 0.8636363744735718\n",
      " Training epoch: 129 - loss: 0.48603811860084534, accuracy: 0.8636363744735718\n",
      " Training epoch: 130 - loss: 0.4851624667644501, accuracy: 0.8636363744735718\n",
      " Training epoch: 131 - loss: 0.4842948019504547, accuracy: 0.8636363744735718\n",
      " Training epoch: 132 - loss: 0.48343613743782043, accuracy: 0.8636363744735718\n",
      " Training epoch: 133 - loss: 0.4825860857963562, accuracy: 0.8636363744735718\n",
      " Training epoch: 134 - loss: 0.481744647026062, accuracy: 0.8636363744735718\n",
      " Training epoch: 135 - loss: 0.48091182112693787, accuracy: 0.8636363744735718\n",
      " Training epoch: 136 - loss: 0.480087548494339, accuracy: 0.8636363744735718\n",
      " Training epoch: 137 - loss: 0.47927066683769226, accuracy: 0.8636363744735718\n",
      " Training epoch: 138 - loss: 0.4784625172615051, accuracy: 0.8636363744735718\n",
      " Training epoch: 139 - loss: 0.47766152024269104, accuracy: 0.8636363744735718\n",
      " Training epoch: 140 - loss: 0.4768686294555664, accuracy: 0.8636363744735718\n",
      " Training epoch: 141 - loss: 0.476082980632782, accuracy: 0.8636363744735718\n",
      " Training epoch: 142 - loss: 0.4753052890300751, accuracy: 0.8636363744735718\n",
      " Training epoch: 143 - loss: 0.47453439235687256, accuracy: 0.8636363744735718\n",
      " Training epoch: 144 - loss: 0.47377079725265503, accuracy: 0.875\n",
      " Training epoch: 145 - loss: 0.4730142652988434, accuracy: 0.875\n",
      " Training epoch: 146 - loss: 0.47226473689079285, accuracy: 0.875\n",
      " Training epoch: 147 - loss: 0.4715222418308258, accuracy: 0.875\n",
      " Training epoch: 148 - loss: 0.47078654170036316, accuracy: 0.875\n",
      " Training epoch: 149 - loss: 0.4700576961040497, accuracy: 0.875\n",
      " Training epoch: 150 - loss: 0.46933504939079285, accuracy: 0.875\n",
      " Training epoch: 151 - loss: 0.4686189889907837, accuracy: 0.875\n",
      " Training epoch: 152 - loss: 0.4679095447063446, accuracy: 0.875\n",
      " Training epoch: 153 - loss: 0.46720579266548157, accuracy: 0.875\n",
      " Training epoch: 154 - loss: 0.46650826930999756, accuracy: 0.875\n",
      " Training epoch: 155 - loss: 0.4658173620700836, accuracy: 0.875\n",
      " Training epoch: 156 - loss: 0.46513253450393677, accuracy: 0.875\n",
      " Training epoch: 157 - loss: 0.4644535779953003, accuracy: 0.875\n",
      " Training epoch: 158 - loss: 0.46378061175346375, accuracy: 0.875\n",
      " Training epoch: 159 - loss: 0.4631131887435913, accuracy: 0.875\n",
      " Training epoch: 160 - loss: 0.4624510705471039, accuracy: 0.875\n",
      " Training epoch: 161 - loss: 0.46179503202438354, accuracy: 0.875\n",
      " Training epoch: 162 - loss: 0.4611445367336273, accuracy: 0.875\n",
      " Training epoch: 163 - loss: 0.46049878001213074, accuracy: 0.875\n",
      " Training epoch: 164 - loss: 0.45985907316207886, accuracy: 0.875\n",
      " Training epoch: 165 - loss: 0.459224671125412, accuracy: 0.875\n",
      " Training epoch: 166 - loss: 0.4585950970649719, accuracy: 0.875\n",
      " Training epoch: 167 - loss: 0.4579709768295288, accuracy: 0.875\n",
      " Training epoch: 168 - loss: 0.4573516845703125, accuracy: 0.875\n",
      " Training epoch: 169 - loss: 0.4567376971244812, accuracy: 0.875\n",
      " Training epoch: 170 - loss: 0.4561280310153961, accuracy: 0.875\n",
      " Training epoch: 171 - loss: 0.45552366971969604, accuracy: 0.875\n",
      " Training epoch: 172 - loss: 0.4549238085746765, accuracy: 0.8863636255264282\n",
      " Training epoch: 173 - loss: 0.4543290436267853, accuracy: 0.8863636255264282\n",
      " Training epoch: 174 - loss: 0.45373934507369995, accuracy: 0.8977272510528564\n",
      " Training epoch: 175 - loss: 0.45315420627593994, accuracy: 0.9090909361839294\n",
      " Training epoch: 176 - loss: 0.45257341861724854, accuracy: 0.9090909361839294\n",
      " Training epoch: 177 - loss: 0.45199671387672424, accuracy: 0.9090909361839294\n",
      " Training epoch: 178 - loss: 0.45142486691474915, accuracy: 0.9090909361839294\n",
      " Training epoch: 179 - loss: 0.4508570730686188, accuracy: 0.9090909361839294\n",
      " Training epoch: 180 - loss: 0.45029380917549133, accuracy: 0.9090909361839294\n",
      " Training epoch: 181 - loss: 0.44973495602607727, accuracy: 0.9090909361839294\n",
      " Training epoch: 182 - loss: 0.4491802453994751, accuracy: 0.9090909361839294\n",
      " Training epoch: 183 - loss: 0.4486296474933624, accuracy: 0.9090909361839294\n",
      " Training epoch: 184 - loss: 0.4480831027030945, accuracy: 0.9090909361839294\n",
      " Training epoch: 185 - loss: 0.4475409686565399, accuracy: 0.9090909361839294\n",
      " Training epoch: 186 - loss: 0.4470024108886719, accuracy: 0.9090909361839294\n",
      " Training epoch: 187 - loss: 0.4464682638645172, accuracy: 0.9090909361839294\n",
      " Training epoch: 188 - loss: 0.4459379315376282, accuracy: 0.9090909361839294\n",
      " Training epoch: 189 - loss: 0.44541117548942566, accuracy: 0.9090909361839294\n",
      " Training epoch: 190 - loss: 0.44488877058029175, accuracy: 0.9090909361839294\n",
      " Training epoch: 191 - loss: 0.44437000155448914, accuracy: 0.9090909361839294\n",
      " Training epoch: 192 - loss: 0.443854421377182, accuracy: 0.9090909361839294\n",
      " Training epoch: 193 - loss: 0.44334304332733154, accuracy: 0.9090909361839294\n",
      " Training epoch: 194 - loss: 0.4428353011608124, accuracy: 0.9090909361839294\n",
      " Training epoch: 195 - loss: 0.4423310458660126, accuracy: 0.9090909361839294\n",
      " Training epoch: 196 - loss: 0.44183072447776794, accuracy: 0.9090909361839294\n",
      " Training epoch: 197 - loss: 0.44133350253105164, accuracy: 0.9090909361839294\n",
      " Training epoch: 198 - loss: 0.4408404231071472, accuracy: 0.9090909361839294\n",
      " Training epoch: 199 - loss: 0.44035008549690247, accuracy: 0.9090909361839294\n",
      "\n",
      "\n",
      " *****Test Accuracy: 0.8920000195503235*****\n"
     ]
    }
   ],
   "source": [
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate_placeholder).minimize(cost)\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate=0.01).minimize(cost)\n",
    "batch_size = 128\n",
    "epochs = 200\n",
    "with tf.Session() as sess:\n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch_i in range(epochs):\n",
    "        #learning_rate = 1e-3 #if epoch_i < 80 else 5e-4\n",
    "        # get batches\n",
    "        for batch_features, batch_labels in get_batches(batch_size, norm_train_data, train_labels):\n",
    "            _, score, loss = sess.run([optimizer, accuracy, cost], feed_dict={features:batch_features, \n",
    "                                                                              labels:batch_labels})\n",
    "        print(f\" Training epoch: {epoch_i} - loss: {loss}, accuracy: {score}\")\n",
    "    test_accuracy = sess.run(accuracy, feed_dict={features:norm_test_data, labels:test_labels})\n",
    "    print(f\"\\n\\n *****Test Accuracy: {test_accuracy}*****\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** ADAGRAD manages to increase test accuracy up to 89% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN with hidden layer\n",
    "\n",
    "So far we used a linear neural network since we haven't use any hidden layer. By using a hidden layer with a non-linear activation function such as a RELU we can enhance the performance of our model since now we will be able to fit a non linear function. \n",
    "\n",
    "![](figures/figure6.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "learning_rate = 0.0008\n",
    "training_epochs = 40\n",
    "batch_size = 128  # Decrease batch size if you don't have enough memory\n",
    "display_step = 1\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "n_hidden_layer = 256 # layer number of features\n",
    "\n",
    "## Store layers weight & bias\n",
    "weights = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_input, n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "## tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, 28, 28, 1])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "x_flat = tf.reshape(x, [-1, n_input])\n",
    "\n",
    "# Hidden layer with RELU activation\n",
    "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer']), biases['hidden_layer'])\n",
    "layer_1 = tf.nn.relu(layer_1)\n",
    "# Output layer with linear activation\n",
    "logits = tf.matmul(layer_1, weights['out']) + biases['out']\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "## ******************************************************************\n",
    "## Test model\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - cost= 47.17692565917969, training_accuracy: 0.34375\n",
      "Epoch: 2 - cost= 23.748462677001953, training_accuracy: 0.5\n",
      "Epoch: 3 - cost= 20.165477752685547, training_accuracy: 0.5859375\n",
      "Epoch: 4 - cost= 22.072261810302734, training_accuracy: 0.6328125\n",
      "Epoch: 5 - cost= 18.01043701171875, training_accuracy: 0.6796875\n",
      "Epoch: 6 - cost= 12.763065338134766, training_accuracy: 0.7265625\n",
      "Epoch: 7 - cost= 18.481178283691406, training_accuracy: 0.6328125\n",
      "Epoch: 8 - cost= 16.535354614257812, training_accuracy: 0.6796875\n",
      "Epoch: 9 - cost= 10.348670959472656, training_accuracy: 0.734375\n",
      "Epoch: 10 - cost= 10.897256851196289, training_accuracy: 0.78125\n",
      "Epoch: 11 - cost= 6.198635101318359, training_accuracy: 0.828125\n",
      "Epoch: 12 - cost= 9.42302131652832, training_accuracy: 0.7734375\n",
      "Epoch: 13 - cost= 6.389941215515137, training_accuracy: 0.8125\n",
      "Epoch: 14 - cost= 12.300577163696289, training_accuracy: 0.7578125\n",
      "Epoch: 15 - cost= 10.338988304138184, training_accuracy: 0.75\n",
      "Epoch: 16 - cost= 7.37807559967041, training_accuracy: 0.8515625\n",
      "Epoch: 17 - cost= 10.90893268585205, training_accuracy: 0.765625\n",
      "Epoch: 18 - cost= 4.997015476226807, training_accuracy: 0.859375\n",
      "Epoch: 19 - cost= 8.354593276977539, training_accuracy: 0.7890625\n",
      "Epoch: 20 - cost= 7.372138500213623, training_accuracy: 0.8125\n",
      "Epoch: 21 - cost= 13.449203491210938, training_accuracy: 0.7109375\n",
      "Epoch: 22 - cost= 3.6156392097473145, training_accuracy: 0.8515625\n",
      "Epoch: 23 - cost= 8.472396850585938, training_accuracy: 0.859375\n",
      "Epoch: 24 - cost= 10.854408264160156, training_accuracy: 0.7734375\n",
      "Epoch: 25 - cost= 6.204185485839844, training_accuracy: 0.8671875\n",
      "Epoch: 26 - cost= 7.356415271759033, training_accuracy: 0.8125\n",
      "Epoch: 27 - cost= 8.655692100524902, training_accuracy: 0.7890625\n",
      "Epoch: 28 - cost= 6.120683670043945, training_accuracy: 0.8515625\n",
      "Epoch: 29 - cost= 9.701385498046875, training_accuracy: 0.8203125\n",
      "Epoch: 30 - cost= 7.886369228363037, training_accuracy: 0.8046875\n",
      "Epoch: 31 - cost= 7.506278038024902, training_accuracy: 0.796875\n",
      "Epoch: 32 - cost= 10.699811935424805, training_accuracy: 0.7734375\n",
      "Epoch: 33 - cost= 6.664682388305664, training_accuracy: 0.8359375\n",
      "Epoch: 34 - cost= 5.017055988311768, training_accuracy: 0.8671875\n",
      "Epoch: 35 - cost= 9.402090072631836, training_accuracy: 0.8515625\n",
      "Epoch: 36 - cost= 4.575145244598389, training_accuracy: 0.84375\n",
      "Epoch: 37 - cost= 5.841904640197754, training_accuracy: 0.875\n",
      "Epoch: 38 - cost= 5.001708984375, training_accuracy: 0.875\n",
      "Epoch: 39 - cost= 4.394211292266846, training_accuracy: 0.8515625\n",
      "Epoch: 40 - cost= 5.382609844207764, training_accuracy: 0.78125\n",
      "Optimization Finished!\n",
      "Accuracy: 0.8534\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        total_batch = int(mnist_data.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist_data.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        # Display loss per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            c = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            train_score = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "            print(f\"Epoch: {epoch+1} - cost= {c}, training_accuracy: {train_score}\")\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    # Decrease test_size if you don't have enough memory\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist_data.test.images, y: mnist_data.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** We can clearly see that the model reaches a good accuracy faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model\n",
    "\n",
    "The model we have been using so far is simple, but more complex models could take hours to train. Therefore, it would be handy if we save our model's parameters to keep them training.\n",
    "\n",
    "**Note:** Remember to always set the name of the tensors in your model, this way when you want to load a previous checkpoint you won't get any error. The reason is that, if you don't set the name of the tensors, Tensorflow sets them automatically with a name based on the order of the definition. \n",
    "\n",
    "> TensorFlow uses a string identifier for Tensors and Operations called name. If a name is not given, TensorFlow will create one automatically. TensorFlow will give the first node the name <Type>, and then give the name <Type>_<number> for the subsequent nodes. Let's see how this can affect loading a model with a different order of weights and bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "learning_rate = 0.0008\n",
    "training_epochs = 20\n",
    "batch_size = 128  # Decrease batch size if you don't have enough memory\n",
    "display_step = 1\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)\n",
    "\n",
    "n_hidden_layer = 256 # layer number of features\n",
    "\n",
    "## Store layers weight & bias\n",
    "weights = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_input, n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "## tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, 28, 28, 1])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "x_flat = tf.reshape(x, [-1, n_input])\n",
    "\n",
    "# Hidden layer with RELU activation\n",
    "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer']), biases['hidden_layer'])\n",
    "layer_1 = tf.nn.relu(layer_1)\n",
    "# Output layer with linear activation\n",
    "logits = tf.matmul(layer_1, weights['out']) + biases['out']\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Saving model \n",
    "save_file = './train_model.ckpt'\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "## ******************************************************************\n",
    "## Test model\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - cost= 39.823604583740234, training_accuracy: 0.390625\n",
      "Epoch: 2 - cost= 34.43508529663086, training_accuracy: 0.4453125\n",
      "Epoch: 3 - cost= 25.481521606445312, training_accuracy: 0.5390625\n",
      "Epoch: 4 - cost= 23.194133758544922, training_accuracy: 0.5703125\n",
      "Epoch: 5 - cost= 20.886192321777344, training_accuracy: 0.640625\n",
      "Epoch: 6 - cost= 11.201963424682617, training_accuracy: 0.6953125\n",
      "Epoch: 7 - cost= 13.850841522216797, training_accuracy: 0.6484375\n",
      "Epoch: 8 - cost= 12.24250602722168, training_accuracy: 0.734375\n",
      "Epoch: 9 - cost= 12.310770034790039, training_accuracy: 0.703125\n",
      "Epoch: 10 - cost= 13.592023849487305, training_accuracy: 0.78125\n",
      "Epoch: 11 - cost= 8.293068885803223, training_accuracy: 0.7890625\n",
      "Epoch: 12 - cost= 13.679699897766113, training_accuracy: 0.7578125\n",
      "Epoch: 13 - cost= 7.100655555725098, training_accuracy: 0.7890625\n",
      "Epoch: 14 - cost= 9.64814281463623, training_accuracy: 0.796875\n",
      "Epoch: 15 - cost= 8.416259765625, training_accuracy: 0.8125\n",
      "Epoch: 16 - cost= 15.71020793914795, training_accuracy: 0.71875\n",
      "Epoch: 17 - cost= 6.184442520141602, training_accuracy: 0.8359375\n",
      "Epoch: 18 - cost= 6.819659233093262, training_accuracy: 0.8046875\n",
      "Epoch: 19 - cost= 6.732131481170654, training_accuracy: 0.859375\n",
      "Epoch: 20 - cost= 7.601059913635254, training_accuracy: 0.8046875\n",
      "Optimization Finished!\n",
      "Accuracy: 0.8098\n",
      "Trained Model Saved.\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        total_batch = int(mnist_data.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist_data.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        # Display loss per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            c = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "            train_score = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "            print(f\"Epoch: {epoch+1} - cost= {c}, training_accuracy: {train_score}\")\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    # Decrease test_size if you don't have enough memory\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist_data.test.images, y: mnist_data.test.labels}))\n",
    "    \n",
    "    # Save the model\n",
    "    saver.save(sess, save_file) # Saves the whole session\n",
    "    print('Trained Model Saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./train_model.ckpt\n",
      "Test Accuracy: 0.8098000288009644\n"
     ]
    }
   ],
   "source": [
    "## Restore from saved model\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, save_file)\n",
    "\n",
    "    test_accuracy = sess.run(\n",
    "        accuracy,\n",
    "        feed_dict={x: mnist_data.test.images, y: mnist_data.test.labels})\n",
    "\n",
    "print('Test Accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
